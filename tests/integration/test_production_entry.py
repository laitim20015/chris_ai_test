#!/usr/bin/env python3
"""
ç”Ÿç”¢å…¥å£ç«¯åˆ°ç«¯æ¸¬è©¦
Production Entry End-to-End Test

æ¸¬è©¦çœŸæ­£çš„DocumentProcessor.process_document()å…¥å£ï¼Œ
ç¢ºä¿ç”Ÿç”¢ç’°å¢ƒçš„APIä¸€è‡´æ€§å’ŒåŠŸèƒ½å®Œæ•´æ€§ã€‚

èˆ‡complete_end_to_end_test.pyçš„å€åˆ¥ï¼š
- complete_end_to_end_test.py: ç›´æ¥èª¿ç”¨å„å€‹çµ„ä»¶ï¼ˆç¹éDocumentProcessorï¼‰
- test_production_entry.py: æ¸¬è©¦çœŸæ­£çš„ç”Ÿç”¢å…¥å£DocumentProcessor.process_document()
"""

import sys
import os
import asyncio
import pytest
from pathlib import Path
from datetime import datetime

# æ·»åŠ srcåˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent.parent / "src"))

from src.main import DocumentProcessor
from src.config.settings import get_settings
from src.config.logging_config import get_logger

logger = get_logger("test_production_entry")

class TestProductionEntry:
    """ç”Ÿç”¢å…¥å£æ¸¬è©¦é¡"""
    
    @pytest.fixture(autouse=True)
    def setup(self):
        """æ¸¬è©¦è¨­ç½®"""
        self.processor = DocumentProcessor()
        self.test_file = Path("tests/fixtures/documents/Workflows-sample.pdf")
        self.output_dir = Path("test_output/production_entry")
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # ç¢ºä¿æ¸¬è©¦æ–‡ä»¶å­˜åœ¨
        if not self.test_file.exists():
            pytest.skip(f"æ¸¬è©¦æ–‡ä»¶ä¸å­˜åœ¨: {self.test_file}")
    
    def test_document_processor_basic_functionality(self):
        """æ¸¬è©¦DocumentProcessoråŸºæœ¬åŠŸèƒ½"""
        
        # æ¸¬è©¦åˆå§‹åŒ–
        assert self.processor is not None
        assert hasattr(self.processor, 'process_document')
        assert hasattr(self.processor, '_perform_association_analysis')
        
        # æ¸¬è©¦çµ„ä»¶åˆå§‹åŒ–
        assert self.processor.parser_factory is not None
        assert self.processor.caption_detector is not None
        assert self.processor.spatial_analyzer is not None
        assert self.processor.semantic_analyzer is not None
        assert self.processor.association_scorer is not None
        assert self.processor.markdown_generator is not None
    
    def test_process_document_success_flow(self):
        """æ¸¬è©¦process_documentæˆåŠŸæµç¨‹"""
        
        result = self.processor.process_document(
            file_path=str(self.test_file),
            output_dir=str(self.output_dir),
            template_name="enhanced.md.j2",
            save_associations=True
        )
        
        # é©—è­‰è¿”å›çµæœçµæ§‹
        assert isinstance(result, dict)
        assert "success" in result
        assert "output_files" in result
        assert "statistics" in result
        
        # é©—è­‰æˆåŠŸç‹€æ…‹
        if not result["success"]:
            logger.error(f"è™•ç†å¤±æ•—: {result.get('error', 'Unknown error')}")
            pytest.fail(f"DocumentProcessor.process_document() å¤±æ•—: {result.get('error')}")
        
        assert result["success"] is True
        
        # é©—è­‰è¼¸å‡ºæ–‡ä»¶
        output_files = result["output_files"]
        assert "markdown" in output_files
        assert "associations" in output_files
        
        # æª¢æŸ¥æ–‡ä»¶æ˜¯å¦ç¢ºå¯¦è¢«å‰µå»º
        markdown_file = Path(output_files["markdown"])
        associations_file = Path(output_files["associations"])
        
        assert markdown_file.exists(), f"Markdownæ–‡ä»¶æœªå‰µå»º: {markdown_file}"
        assert associations_file.exists(), f"é—œè¯æ–‡ä»¶æœªå‰µå»º: {associations_file}"
        
        # é©—è­‰çµ±è¨ˆä¿¡æ¯
        stats = result["statistics"]
        assert "total_text_blocks" in stats
        assert "total_images" in stats
        assert "total_associations" in stats
        assert "processing_time" in stats
        
        # åŸºæœ¬çµ±è¨ˆæª¢æŸ¥
        assert stats["total_text_blocks"] > 0, "æ‡‰è©²æœ‰æ–‡æœ¬å¡Š"
        assert stats["total_images"] > 0, "æ‡‰è©²æœ‰åœ–ç‰‡"
        assert stats["processing_time"] > 0, "è™•ç†æ™‚é–“æ‡‰è©²å¤§æ–¼0"
        
        logger.info(f"âœ… ç”Ÿç”¢å…¥å£æ¸¬è©¦æˆåŠŸ:")
        logger.info(f"  - è™•ç†æ™‚é–“: {stats['processing_time']:.2f}ç§’")
        logger.info(f"  - æ–‡æœ¬å¡Š: {stats['total_text_blocks']}")
        logger.info(f"  - åœ–ç‰‡: {stats['total_images']}")
        logger.info(f"  - é—œè¯: {stats['total_associations']}")
    
    def test_process_document_error_handling(self):
        """æ¸¬è©¦process_documentéŒ¯èª¤è™•ç†"""
        
        # æ¸¬è©¦ä¸å­˜åœ¨çš„æ–‡ä»¶
        result = self.processor.process_document(
            file_path="non_existent_file.pdf",
            output_dir=str(self.output_dir)
        )
        
        assert isinstance(result, dict)
        assert "success" in result
        assert result["success"] is False
        assert "error" in result
        
        logger.info(f"âœ… éŒ¯èª¤è™•ç†æ¸¬è©¦é€šé: {result['error']}")
    
    def test_process_document_with_different_templates(self):
        """æ¸¬è©¦ä¸åŒæ¨¡æ¿çš„è™•ç†"""
        
        templates = ["basic.md.j2", "enhanced.md.j2"]
        
        for template in templates:
            result = self.processor.process_document(
                file_path=str(self.test_file),
                output_dir=str(self.output_dir / template.replace('.md.j2', '')),
                template_name=template,
                save_associations=False
            )
            
            assert result["success"] is True, f"æ¨¡æ¿ {template} è™•ç†å¤±æ•—"
            
            # æª¢æŸ¥è¼¸å‡ºæ–‡ä»¶
            markdown_file = Path(result["output_files"]["markdown"])
            assert markdown_file.exists(), f"æ¨¡æ¿ {template} çš„Markdownæ–‡ä»¶æœªå‰µå»º"
            
            logger.info(f"âœ… æ¨¡æ¿ {template} æ¸¬è©¦é€šé")
    
    def test_api_consistency_with_end_to_end_test(self):
        """æ¸¬è©¦èˆ‡ç«¯åˆ°ç«¯æ¸¬è©¦çš„APIä¸€è‡´æ€§"""
        
        # é‹è¡Œç”Ÿç”¢å…¥å£
        result = self.processor.process_document(
            file_path=str(self.test_file),
            output_dir=str(self.output_dir / "consistency_test"),
            template_name="enhanced.md.j2",
            save_associations=True
        )
        
        assert result["success"] is True
        
        # æª¢æŸ¥é—œè¯åˆ†æçµæœçš„çµæ§‹
        stats = result["statistics"]
        
        # é©—è­‰é—œè¯åˆ†ææ˜¯å¦æ­£å¸¸å·¥ä½œ
        if "total_associations" in stats and stats["total_associations"] > 0:
            logger.info("âœ… é—œè¯åˆ†æåŠŸèƒ½æ­£å¸¸")
        else:
            logger.warning("âš ï¸ æœªæª¢æ¸¬åˆ°é—œè¯åˆ†æçµæœ")
        
        # æª¢æŸ¥é«˜è³ªé‡é—œè¯
        if "high_quality_associations" in stats:
            logger.info(f"âœ… é«˜è³ªé‡é—œè¯: {stats['high_quality_associations']}")
        
        # æª¢æŸ¥å¹³å‡é—œè¯åˆ†æ•¸
        if "average_association_score" in stats:
            avg_score = stats["average_association_score"]
            assert 0 <= avg_score <= 1, f"å¹³å‡é—œè¯åˆ†æ•¸æ‡‰åœ¨0-1ä¹‹é–“: {avg_score}"
            logger.info(f"âœ… å¹³å‡é—œè¯åˆ†æ•¸: {avg_score:.3f}")
    
    def test_performance_benchmark(self):
        """åŸºæœ¬æ€§èƒ½åŸºæº–æ¸¬è©¦"""
        
        start_time = datetime.now()
        
        result = self.processor.process_document(
            file_path=str(self.test_file),
            output_dir=str(self.output_dir / "performance_test"),
            template_name="enhanced.md.j2",
            save_associations=True
        )
        
        end_time = datetime.now()
        total_time = (end_time - start_time).total_seconds()
        
        assert result["success"] is True
        
        # æ€§èƒ½åŸºæº–ï¼šè™•ç†1MBæ–‡ä»¶æ‡‰åœ¨30ç§’å…§å®Œæˆï¼ˆè¦æ ¼è¦æ±‚ï¼‰
        file_size_mb = self.test_file.stat().st_size / (1024 * 1024)
        max_time = 30.0  # ç§’
        
        logger.info(f"ğŸ“Š æ€§èƒ½æ¸¬è©¦çµæœ:")
        logger.info(f"  - æ–‡ä»¶å¤§å°: {file_size_mb:.2f} MB")
        logger.info(f"  - è™•ç†æ™‚é–“: {total_time:.2f} ç§’")
        logger.info(f"  - æ€§èƒ½åŸºæº–: < {max_time} ç§’")
        
        if total_time <= max_time:
            logger.info("âœ… æ€§èƒ½ç¬¦åˆè¦æ ¼è¦æ±‚")
        else:
            logger.warning(f"âš ï¸ æ€§èƒ½ç•¥ä½æ–¼é æœŸï¼Œä½†ä»å¯æ¥å—")
        
        # è¨˜éŒ„åˆ°çµ±è¨ˆä¸­ç”¨æ–¼æ€§èƒ½ç›£æ§
        stats = result["statistics"]
        assert "processing_time" in stats
        assert abs(stats["processing_time"] - total_time) < 1.0  # å…è¨±1ç§’èª¤å·®


# å‘½ä»¤è¡ŒåŸ·è¡Œæ”¯æŒ
if __name__ == "__main__":
    pytest.main([__file__, "-v", "--tb=short"])
