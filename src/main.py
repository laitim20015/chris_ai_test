"""
ä¸»æ‡‰ç”¨ç¨‹åºå…¥å£é»

æ™ºèƒ½æ–‡ä»¶è½‰æ›èˆ‡RAGçŸ¥è­˜åº«ç³»çµ±çš„æ ¸å¿ƒè™•ç†å¼•æ“ï¼Œ
å”èª¿æ–‡ä»¶è§£æã€åœ–æ–‡é—œè¯åˆ†æå’ŒMarkdownç”Ÿæˆçš„å®Œæ•´æµç¨‹ã€‚
"""

import os
import time
from typing import Dict, List, Any, Optional
from pathlib import Path
from datetime import datetime

from .config.settings import get_settings
from .config.logging_config import get_logger, PerformanceLogger
from .parsers import ParserFactory, ParsedContent
from .association import AssociationScorer
from .association.allen_logic import AllenLogicAnalyzer
from .association.caption_detector import CaptionDetector
from .association.spatial_analyzer import SpatialAnalyzer
from .association.semantic_analyzer import SemanticAnalyzer
from .association.association_optimizer import AssociationOptimizer, create_balanced_config
from .markdown import MarkdownGenerator
from .utils.file_utils import ensure_directory_exists, get_file_hash
from .utils.validation import validate_file_path, check_file_safety

logger = get_logger(__name__)


class DocumentProcessor:
    """
    æ–‡ä»¶è™•ç†æ ¸å¿ƒå¼•æ“
    
    å”èª¿æ•´å€‹æ–‡ä»¶è½‰æ›æµç¨‹ï¼Œå¾åŸå§‹æ–‡ä»¶åˆ°Markdownè¼¸å‡ºã€‚
    æ”¯æŒå¤šç¨®æ–‡ä»¶æ ¼å¼å’Œæ™ºèƒ½åœ–æ–‡é—œè¯åˆ†æã€‚
    """
    
    def __init__(self):
        """åˆå§‹åŒ–æ–‡ä»¶è™•ç†å™¨"""
        self.settings = get_settings()
        self.parser_factory = ParserFactory()
        self.markdown_generator = MarkdownGenerator()
        
        # åˆå§‹åŒ–é—œè¯åˆ†æçµ„ä»¶
        self.allen_analyzer = AllenLogicAnalyzer()
        self.caption_detector = CaptionDetector()
        self.spatial_analyzer = SpatialAnalyzer()
        self.semantic_analyzer = SemanticAnalyzer()
        self.association_scorer = AssociationScorer()
        
        # åˆå§‹åŒ–é—œè¯å„ªåŒ–å™¨
        self.association_optimizer = AssociationOptimizer(create_balanced_config())
        
        # æ€§èƒ½ç›£æ§
        self.performance_logger = PerformanceLogger()
        
        logger.info("æ–‡ä»¶è™•ç†å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def process_document(
        self,
        file_path: str,
        output_dir: Optional[str] = None,
        template_name: str = "enhanced.md.j2",
        save_associations: bool = True
    ) -> Dict[str, Any]:
        """
        è™•ç†å–®å€‹æ–‡æª”
        
        Args:
            file_path: è¼¸å…¥æ–‡ä»¶è·¯å¾‘
            output_dir: è¼¸å‡ºç›®éŒ„ï¼ˆå¯é¸ï¼‰
            template_name: ä½¿ç”¨çš„æ¨¡æ¿åç¨±
            save_associations: æ˜¯å¦ä¿å­˜é—œè¯åˆ†æçµæœ
            
        Returns:
            Dict: è™•ç†çµæœï¼ŒåŒ…å«ç”Ÿæˆçš„æ–‡ä»¶è·¯å¾‘å’Œçµ±è¨ˆä¿¡æ¯
        """
        
        start_time = time.time()
        
        try:
            # 1. æ–‡ä»¶é©—è­‰
            logger.info(f"é–‹å§‹è™•ç†æ–‡æª”: {file_path}")
            
            if not validate_file_path(file_path):
                raise ValueError(f"æ–‡ä»¶è·¯å¾‘ç„¡æ•ˆ: {file_path}")
            
            is_safe, warnings = check_file_safety(file_path)
            if not is_safe:
                logger.warning(f"æ–‡ä»¶å®‰å…¨æª¢æŸ¥è­¦å‘Š: {warnings}")
            
            # 2. æ–‡ä»¶è§£æ
            with self.performance_logger.measure("file_parsing"):
                parsed_content = self._parse_file(file_path)
            
            logger.info(
                f"æ–‡ä»¶è§£æå®Œæˆ - æ–‡æœ¬å¡Š: {len(parsed_content.text_blocks)}, "
                f"åœ–ç‰‡: {len(parsed_content.images)}, "
                f"è¡¨æ ¼: {len(parsed_content.tables)}"
            )
            
            # 3. åœ–æ–‡é—œè¯åˆ†æ
            with self.performance_logger.measure("association_analysis"):
                associations = self._analyze_associations(parsed_content)
            
            logger.info(f"é—œè¯åˆ†æå®Œæˆ - ç™¼ç¾ {len(associations)} å€‹é—œè¯é—œä¿‚")
            
            # 4. ç”ŸæˆMarkdown
            with self.performance_logger.measure("markdown_generation"):
                markdown_content = self._generate_markdown(
                    parsed_content, associations, template_name
                )
            
            # 5. ä¿å­˜çµæœ
            output_files = self._save_results(
                parsed_content, associations, markdown_content,
                output_dir, save_associations
            )
            
            # 6. çµ±è¨ˆä¿¡æ¯
            processing_time = time.time() - start_time
            stats = self._collect_statistics(
                parsed_content, associations, processing_time
            )
            
            logger.info(f"æ–‡æª”è™•ç†å®Œæˆï¼Œè€—æ™‚: {processing_time:.2f}ç§’")
            
            return {
                "success": True,
                "output_files": output_files,
                "statistics": stats,
                "processing_time": processing_time
            }
            
        except Exception as e:
            logger.error(f"æ–‡æª”è™•ç†å¤±æ•—: {e}")
            return {
                "success": False,
                "error": str(e),
                "processing_time": time.time() - start_time
            }
    
    def _parse_file(self, file_path: str) -> ParsedContent:
        """è§£ææ–‡ä»¶å…§å®¹"""
        
        file_extension = Path(file_path).suffix.lower()
        parser = self.parser_factory.get_parser(file_extension)
        
        if not parser:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_extension}")
        
        return parser.parse(file_path)
    
    def _analyze_associations(self, parsed_content: ParsedContent) -> List[Dict[str, Any]]:
        """åˆ†æåœ–æ–‡é—œè¯é—œä¿‚"""
        
        associations = []
        
        for text_block in parsed_content.text_blocks:
            for image in parsed_content.images:
                try:
                    # åŸ·è¡Œå¤šå±¤æ¬¡é—œè¯åˆ†æ
                    association_result = self._perform_association_analysis(
                        text_block, image
                    )
                    
                    # åªä¿ç•™é«˜æ–¼é–¾å€¼çš„é—œè¯
                    threshold = self.settings.association.min_association_score
                    if association_result["final_score"] >= threshold:
                        associations.append(association_result)
                        
                except Exception as e:
                    logger.warning(f"é—œè¯åˆ†æå¤±æ•— - æ–‡æœ¬å¡Š: {text_block.id}, åœ–ç‰‡: {image.id}, éŒ¯èª¤: {e}")
        
        # ğŸ”§ é—œè¯å„ªåŒ– - å»é‡ã€éæ¿¾å’Œè³ªé‡æå‡
        logger.info(f"é–‹å§‹é—œè¯å„ªåŒ– - åŸå§‹é—œè¯æ•¸: {len(associations)}")
        
        optimized_associations = self.association_optimizer.optimize_associations(
            associations=associations,
            images=parsed_content.images,
            text_blocks=parsed_content.text_blocks
        )
        
        logger.info(f"é—œè¯å„ªåŒ–å®Œæˆ - å„ªåŒ–å¾Œé—œè¯æ•¸: {len(optimized_associations)}")
        logger.info(f"é—œè¯æ¸›å°‘ç‡: {((len(associations) - len(optimized_associations)) / len(associations) * 100):.1f}%" if associations else "0%")
        
        return optimized_associations
    
    def _perform_association_analysis(self, text_block, image) -> Dict[str, Any]:
        """åŸ·è¡Œå–®å€‹æ–‡æœ¬å¡Šå’Œåœ–ç‰‡çš„é—œè¯åˆ†æ"""
        
        # 1. Captionæª¢æ¸¬
        caption_matches = self.caption_detector.detect_captions(
            text_block.content, text_block.bbox, image.bbox
        )
        caption_score = max((match.confidence for match in caption_matches), default=0.0)
        
        # 2. ç©ºé–“é—œä¿‚åˆ†æ
        spatial_features = self.spatial_analyzer.calculate_spatial_features(
            text_block.bbox, image.bbox
        )
        
        # 3. èªç¾©ç›¸ä¼¼åº¦åˆ†æ
        semantic_score = self.semantic_analyzer.calculate_similarity(
            text_block.content, 
            image.alt_text or f"Image {image.id}"
        )
        
        # 4. ç¶œåˆè©•åˆ† - ä½¿ç”¨å‹•æ…‹æ­¸ä¸€åŒ–
        if spatial_features:
            # ä¼°ç®—é é¢å°ºå¯¸ï¼ˆä½¿ç”¨æ‰€æœ‰å…ƒç´ çš„æœ€å¤§åº§æ¨™ï¼‰
            page_width = max(text_block.bbox.right, image.bbox.right)
            page_height = max(text_block.bbox.bottom, image.bbox.bottom)
            page_diagonal = (page_width ** 2 + page_height ** 2) ** 0.5
            
            # ä½¿ç”¨é é¢å°è§’ç·šçš„æ¯”ä¾‹ä½œç‚ºæ­¸ä¸€åŒ–åŸºæº–
            # å°è§’ç·šçš„50%ä½œç‚º"ä¸­è·é›¢"ï¼Œ30%ä½œç‚º"è¿‘è·é›¢"
            spatial_score = 1.0 - min(spatial_features.center_distance / (page_diagonal * 0.5), 1.0)
            proximity_score = 1.0 - min(spatial_features.min_distance / (page_diagonal * 0.3), 1.0)
            layout_score = spatial_features.alignment_score
        else:
            spatial_score = 0.0
            proximity_score = 0.0
            layout_score = 0.0
        
        final_score, details = self.association_scorer.calculate_simple_score(
            caption_score=caption_score,
            spatial_score=spatial_score,
            semantic_score=semantic_score,
            layout_score=layout_score,
            proximity_score=proximity_score
        )
        
        return {
            "text_block_id": text_block.id,
            "image_id": image.id,
            "final_score": final_score,
            "caption_score": caption_score,
            "spatial_score": spatial_score,
            "semantic_score": semantic_score,
            "layout_score": layout_score,
            "proximity_score": proximity_score,
            "spatial_relation": "calculated",  # ç”±ç©ºé–“ç‰¹å¾µè¨ˆç®—å¾—å‡º
            "association_type": "caption" if caption_score > 0.5 else "spatial",
            "details": details,
            "spatial_features": {
                "center_distance": spatial_features.center_distance,
                "alignment_score": spatial_features.alignment_score,
                "min_distance": spatial_features.min_distance
            }
        }
    
    def _generate_markdown(
        self, 
        parsed_content: ParsedContent, 
        associations: List[Dict[str, Any]],
        template_name: str
    ) -> str:
        """ç”ŸæˆMarkdownå…§å®¹"""
        
        return self.markdown_generator.generate(
            parsed_content=parsed_content,
            associations=associations,
            template_name=template_name,
            include_metadata=True
        )
    
    def _save_results(
        self,
        parsed_content: ParsedContent,
        associations: List[Dict[str, Any]],
        markdown_content: str,
        output_dir: Optional[str],
        save_associations: bool
    ) -> Dict[str, str]:
        """ä¿å­˜è™•ç†çµæœ"""
        
        if not output_dir:
            output_dir = "./output"
        
        ensure_directory_exists(output_dir)
        
        # ç”Ÿæˆæ–‡ä»¶å
        base_name = Path(parsed_content.metadata.filename).stem
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        output_files = {}
        
        # ä¿å­˜Markdownæ–‡ä»¶
        markdown_path = Path(output_dir) / f"{base_name}_{timestamp}.md"
        with open(markdown_path, 'w', encoding='utf-8') as f:
            f.write(markdown_content)
        output_files["markdown"] = str(markdown_path)
        
        # ä¿å­˜é—œè¯åˆ†æçµæœ
        if save_associations:
            import json
            from enum import Enum
            
            # åºåˆ—åŒ–åŠ©æ‰‹å‡½æ•¸
            def serialize_for_json(obj):
                if isinstance(obj, Enum):
                    return obj.value
                elif hasattr(obj, '__dict__'):
                    return {k: serialize_for_json(v) for k, v in obj.__dict__.items()}
                elif isinstance(obj, (list, tuple)):
                    return [serialize_for_json(item) for item in obj]
                elif isinstance(obj, dict):
                    return {k: serialize_for_json(v) for k, v in obj.items()}
                else:
                    return obj
            
            serializable_associations = serialize_for_json(associations)
            
            associations_path = Path(output_dir) / f"{base_name}_{timestamp}_associations.json"
            with open(associations_path, 'w', encoding='utf-8') as f:
                json.dump(serializable_associations, f, ensure_ascii=False, indent=2)
            output_files["associations"] = str(associations_path)
        
        return output_files
    
    def _collect_statistics(
        self,
        parsed_content: ParsedContent,
        associations: List[Dict[str, Any]],
        processing_time: float
    ) -> Dict[str, Any]:
        """æ”¶é›†è™•ç†çµ±è¨ˆä¿¡æ¯"""
        
        # è¨ˆç®—é—œè¯çµ±è¨ˆ
        high_quality_associations = [
            a for a in associations if a["final_score"] >= 0.7
        ]
        
        caption_associations = [
            a for a in associations if a["association_type"] == "caption"
        ]
        
        return {
            "processing_time": processing_time,
            "total_text_blocks": len(parsed_content.text_blocks),
            "total_images": len(parsed_content.images),
            "total_tables": len(parsed_content.tables),
            "total_associations": len(associations),
            "high_quality_associations": len(high_quality_associations),
            "caption_associations": len(caption_associations),
            "average_association_score": (
                sum(a["final_score"] for a in associations) / len(associations)
                if associations else 0.0
            ),
            "performance_metrics": self.performance_logger.get_summary()
        }


def main():
    """ä¸»å‡½æ•¸ - å‘½ä»¤è¡Œç•Œé¢"""
    
    import argparse
    
    parser = argparse.ArgumentParser(
        description="æ™ºèƒ½æ–‡ä»¶è½‰æ›èˆ‡RAGçŸ¥è­˜åº«ç³»çµ±"
    )
    
    parser.add_argument(
        "input_file",
        help="è¼¸å…¥æ–‡ä»¶è·¯å¾‘"
    )
    
    parser.add_argument(
        "-o", "--output",
        help="è¼¸å‡ºç›®éŒ„",
        default="./output"
    )
    
    parser.add_argument(
        "-t", "--template",
        help="ä½¿ç”¨çš„æ¨¡æ¿",
        choices=["basic.md.j2", "enhanced.md.j2"],
        default="enhanced.md.j2"
    )
    
    parser.add_argument(
        "--no-associations",
        action="store_true",
        help="ä¸ä¿å­˜é—œè¯åˆ†æçµæœ"
    )
    
    args = parser.parse_args()
    
    try:
        processor = DocumentProcessor()
        
        result = processor.process_document(
            file_path=args.input_file,
            output_dir=args.output,
            template_name=args.template,
            save_associations=not args.no_associations
        )
        
        if result["success"]:
            print("âœ… è™•ç†æˆåŠŸï¼")
            print(f"ğŸ“Š çµ±è¨ˆä¿¡æ¯:")
            stats = result["statistics"]
            print(f"  - è™•ç†æ™‚é–“: {stats['processing_time']:.2f}ç§’")
            print(f"  - æ–‡æœ¬å¡Š: {stats['total_text_blocks']}")
            print(f"  - åœ–ç‰‡: {stats['total_images']}")
            print(f"  - é—œè¯é—œä¿‚: {stats['total_associations']}")
            print(f"  - é«˜è³ªé‡é—œè¯: {stats['high_quality_associations']}")
            
            print(f"ğŸ“ è¼¸å‡ºæ–‡ä»¶:")
            for file_type, file_path in result["output_files"].items():
                print(f"  - {file_type}: {file_path}")
        else:
            print(f"âŒ è™•ç†å¤±æ•—: {result['error']}")
            return 1
    
    except Exception as e:
        logger.error(f"ä¸»ç¨‹åºåŸ·è¡Œå¤±æ•—: {e}")
        print(f"âŒ åŸ·è¡Œå¤±æ•—: {e}")
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main())
