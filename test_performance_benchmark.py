#!/usr/bin/env python3
"""
æ€§èƒ½åŸºæº–æ¸¬è©¦
Performance Benchmark Test

æ¸¬è©¦ç©ºé–“åˆ†ææ”¹é€²çš„æ€§èƒ½åŸºæº–ï¼ŒåŒ…æ‹¬ï¼š
1. ä¸åŒè¦æ¨¡æ•¸æ“šçš„æ€§èƒ½æ¸¬è©¦
2. ç·©å­˜æ•ˆæœæ¸¬è©¦
3. è¨˜æ†¶é«”ä½¿ç”¨æ¸¬è©¦
4. ä½µç™¼æ€§èƒ½æ¸¬è©¦
"""

import sys
import os
import time
import random
import threading
from pathlib import Path
from typing import List, Dict, Any
import json
import gc
import psutil

# æ·»åŠ é …ç›®æ ¹ç›®éŒ„åˆ°è·¯å¾‘
sys.path.append(str(Path(__file__).parent))

from src.association.spatial_analyzer import (
    enhanced_spatial_scoring, enhanced_spatial_scoring_optimized,
    get_performance_stats, clear_cache, SpatialAnalyzer
)
from src.association.allen_logic import BoundingBox
from src.association.candidate_ranker import CandidateRanker

def generate_random_bbox(page_width: int = 600, page_height: int = 800) -> BoundingBox:
    """ç”Ÿæˆéš¨æ©Ÿé‚Šç•Œæ¡†"""
    x = random.randint(0, page_width - 100)
    y = random.randint(0, page_height - 100)
    width = random.randint(50, min(200, page_width - x))
    height = random.randint(20, min(100, page_height - y))
    return BoundingBox(x=x, y=y, width=width, height=height)

def generate_test_data(num_pairs: int = 100) -> List[tuple]:
    """ç”Ÿæˆæ¸¬è©¦æ•¸æ“šé›†"""
    test_data = []
    
    for i in range(num_pairs):
        text_bbox = generate_random_bbox()
        image_bbox = generate_random_bbox()
        
        context_info = {
            'layout_type': random.choice(['single_column', 'double_column', 'multi_column']),
            'all_elements': [text_bbox, image_bbox],
            'page_width': 600,
            'page_height': 800
        }
        
        test_data.append((f"pair_{i}", text_bbox, image_bbox, context_info))
    
    return test_data

def measure_memory_usage():
    """æ¸¬é‡ç•¶å‰è¨˜æ†¶é«”ä½¿ç”¨"""
    process = psutil.Process(os.getpid())
    memory_info = process.memory_info()
    return {
        'rss_mb': memory_info.rss / 1024 / 1024,  # å¯¦éš›è¨˜æ†¶é«”ä½¿ç”¨
        'vms_mb': memory_info.vms / 1024 / 1024,  # è™›æ“¬è¨˜æ†¶é«”ä½¿ç”¨
    }

def test_scale_performance():
    """æ¸¬è©¦ä¸åŒè¦æ¨¡æ•¸æ“šçš„æ€§èƒ½"""
    print("ğŸ”¬ è¦æ¨¡æ€§èƒ½æ¸¬è©¦")
    print("=" * 50)
    
    scales = [10, 50, 100, 200, 500]
    results = {}
    
    for scale in scales:
        print(f"\nğŸ“Š æ¸¬è©¦è¦æ¨¡: {scale} å°")
        
        # ç”Ÿæˆæ¸¬è©¦æ•¸æ“š
        test_data = generate_test_data(scale)
        
        # æ¸…ç©ºç·©å­˜
        clear_cache()
        gc.collect()
        
        # æ¸¬é‡åˆå§‹è¨˜æ†¶é«”
        initial_memory = measure_memory_usage()
        
        # åŸ·è¡Œæ¸¬è©¦
        start_time = time.time()
        scores = []
        
        for name, text_bbox, image_bbox, context_info in test_data:
            result = enhanced_spatial_scoring(text_bbox, image_bbox, context_info)
            scores.append(result['final_score'])
        
        end_time = time.time()
        
        # æ¸¬é‡æœ€çµ‚è¨˜æ†¶é«”
        final_memory = measure_memory_usage()
        
        # è¨ˆç®—çµ±è¨ˆ
        total_time = max(end_time - start_time, 0.001)  # é¿å…é™¤é›¶éŒ¯èª¤
        avg_time_per_pair = total_time / scale
        memory_increase = final_memory['rss_mb'] - initial_memory['rss_mb']
        
        results[scale] = {
            'total_time': total_time,
            'avg_time_per_pair': avg_time_per_pair,
            'memory_increase_mb': memory_increase,
            'avg_score': sum(scores) / len(scores),
            'throughput_pairs_per_sec': scale / total_time
        }
        
        print(f"  ç¸½æ™‚é–“: {total_time:.3f}s")
        print(f"  å¹³å‡æ¯å°: {avg_time_per_pair:.4f}s")
        print(f"  è¨˜æ†¶é«”å¢é•·: {memory_increase:.2f} MB")
        print(f"  ååé‡: {results[scale]['throughput_pairs_per_sec']:.1f} pairs/s")
        print(f"  å¹³å‡åˆ†æ•¸: {results[scale]['avg_score']:.3f}")
    
    return results

def test_cache_effectiveness():
    """æ¸¬è©¦ç·©å­˜æ•ˆæœ"""
    print("\nğŸš€ ç·©å­˜æ•ˆæœæ¸¬è©¦")
    print("=" * 50)
    
    # ç”Ÿæˆæ¸¬è©¦æ•¸æ“šï¼ˆè¼ƒå°è¦æ¨¡ï¼Œä¾¿æ–¼é‡è¤‡ä½¿ç”¨ï¼‰
    test_data = generate_test_data(20)
    
    results = {}
    
    # æ¸¬è©¦å ´æ™¯1ï¼šç„¡ç·©å­˜
    print("\n1. ç„¡ç·©å­˜æ€§èƒ½æ¸¬è©¦")
    clear_cache()
    
    start_time = time.time()
    for name, text_bbox, image_bbox, context_info in test_data * 3:  # é‡è¤‡3æ¬¡
        result = enhanced_spatial_scoring(text_bbox, image_bbox, context_info)
    no_cache_time = time.time() - start_time
    
    results['no_cache'] = {
        'time': no_cache_time,
        'requests': len(test_data) * 3
    }
    
    print(f"  ç¸½æ™‚é–“: {no_cache_time:.3f}s")
    print(f"  è«‹æ±‚æ•¸: {results['no_cache']['requests']}")
    print(f"  å¹³å‡æ¯è«‹æ±‚: {no_cache_time / results['no_cache']['requests']:.4f}s")
    
    # æ¸¬è©¦å ´æ™¯2ï¼šæœ‰ç·©å­˜
    print("\n2. æœ‰ç·©å­˜æ€§èƒ½æ¸¬è©¦")
    clear_cache()
    
    start_time = time.time()
    for name, text_bbox, image_bbox, context_info in test_data * 3:  # é‡è¤‡3æ¬¡
        result = enhanced_spatial_scoring_optimized(text_bbox, image_bbox, context_info, enable_cache=True)
    cache_time = time.time() - start_time
    
    # ç²å–ç·©å­˜çµ±è¨ˆ
    cache_stats = get_performance_stats()
    
    results['with_cache'] = {
        'time': cache_time,
        'requests': len(test_data) * 3,
        'cache_stats': cache_stats
    }
    
    print(f"  ç¸½æ™‚é–“: {cache_time:.3f}s")
    print(f"  è«‹æ±‚æ•¸: {results['with_cache']['requests']}")
    print(f"  å¹³å‡æ¯è«‹æ±‚: {cache_time / results['with_cache']['requests']:.4f}s")
    
    if 'overall_stats' in cache_stats:
        overall = cache_stats['overall_stats']
        print(f"  ç·©å­˜å‘½ä¸­ç‡: {overall['overall_hit_rate']:.3f}")
        print(f"  ç·©å­˜è¨˜æ†¶é«”: {overall['total_memory_usage_mb']:.3f} MB")
    
    # è¨ˆç®—æ”¹é€²
    speedup = no_cache_time / max(cache_time, 0.001)
    print(f"\nğŸ’¡ ç·©å­˜æ”¹é€²:")
    print(f"  åŠ é€Ÿæ¯”: {speedup:.2f}x")
    print(f"  æ™‚é–“ç¯€çœ: {((no_cache_time - cache_time) / no_cache_time * 100):.1f}%")
    
    return results

def test_concurrent_performance():
    """æ¸¬è©¦ä½µç™¼æ€§èƒ½"""
    print("\nâš¡ ä½µç™¼æ€§èƒ½æ¸¬è©¦")
    print("=" * 50)
    
    test_data = generate_test_data(10)
    thread_counts = [1, 2, 4, 8]
    results = {}
    
    def worker_function(thread_id: int, data_subset: List, results_list: List):
        """å·¥ä½œç·šç¨‹å‡½æ•¸"""
        thread_results = []
        for name, text_bbox, image_bbox, context_info in data_subset:
            start = time.time()
            result = enhanced_spatial_scoring_optimized(
                text_bbox, image_bbox, context_info, enable_cache=True
            )
            end = time.time()
            thread_results.append({
                'thread_id': thread_id,
                'time': end - start,
                'score': result['final_score']
            })
        results_list.extend(thread_results)
    
    for thread_count in thread_counts:
        print(f"\nğŸ“ˆ {thread_count} ç·šç¨‹æ¸¬è©¦:")
        
        # æ¸…ç©ºç·©å­˜
        clear_cache()
        
        # åˆ†é…æ•¸æ“šçµ¦ç·šç¨‹
        data_per_thread = len(test_data) // thread_count
        threads = []
        all_results = []
        
        start_time = time.time()
        
        for i in range(thread_count):
            start_idx = i * data_per_thread
            end_idx = start_idx + data_per_thread if i < thread_count - 1 else len(test_data)
            data_subset = test_data[start_idx:end_idx]
            
            thread = threading.Thread(
                target=worker_function,
                args=(i, data_subset, all_results)
            )
            threads.append(thread)
            thread.start()
        
        # ç­‰å¾…æ‰€æœ‰ç·šç¨‹å®Œæˆ
        for thread in threads:
            thread.join()
        
        total_time = time.time() - start_time
        
        results[thread_count] = {
            'total_time': total_time,
            'thread_count': thread_count,
            'total_operations': len(all_results),
            'avg_time_per_op': sum(r['time'] for r in all_results) / max(len(all_results), 1),
            'throughput': len(all_results) / max(total_time, 0.001)
        }
        
        print(f"  ç¸½æ™‚é–“: {total_time:.3f}s")
        print(f"  ç¸½æ“ä½œæ•¸: {len(all_results)}")
        print(f"  ååé‡: {results[thread_count]['throughput']:.1f} ops/s")
        print(f"  å¹³å‡å»¶é²: {results[thread_count]['avg_time_per_op']:.4f}s")
    
    return results

def test_algorithm_comparison():
    """æ¸¬è©¦æ–°èˆŠç®—æ³•æ€§èƒ½å°æ¯”"""
    print("\nğŸ†š ç®—æ³•å°æ¯”æ¸¬è©¦")
    print("=" * 50)
    
    test_data = generate_test_data(50)
    
    # å‰µå»ºåˆ†æå™¨å¯¦ä¾‹
    analyzer = SpatialAnalyzer()
    
    results = {}
    
    # æ¸¬è©¦èˆŠç®—æ³•ï¼ˆåŸºç¤ç©ºé–“åˆ†æï¼‰
    print("\n1. åŸºç¤ç©ºé–“åˆ†æ:")
    start_time = time.time()
    old_scores = []
    
    for name, text_bbox, image_bbox, context_info in test_data:
        # ä½¿ç”¨åŸºç¤çš„ç©ºé–“ç‰¹å¾µè¨ˆç®—
        features = analyzer.calculate_spatial_features(text_bbox, image_bbox)
        score = features.alignment_score  # ç°¡åŒ–è©•åˆ†
        old_scores.append(score)
    
    old_time = time.time() - start_time
    
    results['basic_algorithm'] = {
        'time': old_time,
        'avg_score': sum(old_scores) / len(old_scores),
        'operations': len(test_data)
    }
    
    print(f"  æ™‚é–“: {old_time:.3f}s")
    print(f"  å¹³å‡åˆ†æ•¸: {results['basic_algorithm']['avg_score']:.3f}")
    print(f"  ååé‡: {len(test_data) / max(old_time, 0.001):.1f} ops/s")
    
    # æ¸¬è©¦æ–°ç®—æ³•ï¼ˆå¢å¼·ç©ºé–“åˆ†æï¼‰
    print("\n2. å¢å¼·ç©ºé–“åˆ†æ:")
    clear_cache()
    start_time = time.time()
    new_scores = []
    
    for name, text_bbox, image_bbox, context_info in test_data:
        result = enhanced_spatial_scoring(text_bbox, image_bbox, context_info)
        new_scores.append(result['final_score'])
    
    new_time = time.time() - start_time
    
    results['enhanced_algorithm'] = {
        'time': new_time,
        'avg_score': sum(new_scores) / len(new_scores),
        'operations': len(test_data)
    }
    
    print(f"  æ™‚é–“: {new_time:.3f}s")
    print(f"  å¹³å‡åˆ†æ•¸: {results['enhanced_algorithm']['avg_score']:.3f}")
    print(f"  ååé‡: {len(test_data) / max(new_time, 0.001):.1f} ops/s")
    
    # æ¸¬è©¦å„ªåŒ–ç®—æ³•ï¼ˆå¢å¼·+ç·©å­˜ï¼‰
    print("\n3. å¢å¼·ç©ºé–“åˆ†æ + ç·©å­˜:")
    clear_cache()
    start_time = time.time()
    optimized_scores = []
    
    # é‹è¡Œå…©æ¬¡ä¾†æ¸¬è©¦ç·©å­˜æ•ˆæœ
    for name, text_bbox, image_bbox, context_info in test_data * 2:
        result = enhanced_spatial_scoring_optimized(text_bbox, image_bbox, context_info, enable_cache=True)
        optimized_scores.append(result['final_score'])
    
    optimized_time = time.time() - start_time
    
    results['optimized_algorithm'] = {
        'time': optimized_time,
        'avg_score': sum(optimized_scores) / len(optimized_scores),
        'operations': len(optimized_scores),
        'cache_stats': get_performance_stats()
    }
    
    print(f"  æ™‚é–“: {optimized_time:.3f}s")
    print(f"  å¹³å‡åˆ†æ•¸: {results['optimized_algorithm']['avg_score']:.3f}")
    print(f"  ååé‡: {len(optimized_scores) / max(optimized_time, 0.001):.1f} ops/s")
    
    # å°æ¯”åˆ†æ
    print(f"\nğŸ“Š æ€§èƒ½å°æ¯”:")
    basic_throughput = results['basic_algorithm']['operations'] / max(results['basic_algorithm']['time'], 0.001)
    enhanced_throughput = results['enhanced_algorithm']['operations'] / max(results['enhanced_algorithm']['time'], 0.001)
    optimized_throughput = results['optimized_algorithm']['operations'] / max(results['optimized_algorithm']['time'], 0.001)
    
    print(f"  åŸºç¤ç®—æ³• vs å¢å¼·ç®—æ³•: {enhanced_throughput / max(basic_throughput, 0.001):.2f}x")
    print(f"  åŸºç¤ç®—æ³• vs å„ªåŒ–ç®—æ³•: {optimized_throughput / max(basic_throughput, 0.001):.2f}x")
    print(f"  å¢å¼·ç®—æ³• vs å„ªåŒ–ç®—æ³•: {optimized_throughput / max(enhanced_throughput, 0.001):.2f}x")
    
    return results

def run_performance_benchmark():
    """é‹è¡Œå®Œæ•´çš„æ€§èƒ½åŸºæº–æ¸¬è©¦"""
    print("ğŸš€ ç©ºé–“åˆ†ææ€§èƒ½åŸºæº–æ¸¬è©¦")
    print("=" * 70)
    
    start_time = time.time()
    
    # è¨˜éŒ„ç³»çµ±ä¿¡æ¯
    print("ğŸ–¥ï¸  ç³»çµ±ä¿¡æ¯:")
    print(f"  CPU æ ¸å¿ƒæ•¸: {psutil.cpu_count()}")
    print(f"  ç¸½è¨˜æ†¶é«”: {psutil.virtual_memory().total / 1024 / 1024 / 1024:.1f} GB")
    print(f"  Python ç‰ˆæœ¬: {sys.version}")
    
    results = {}
    
    try:
        # 1. è¦æ¨¡æ€§èƒ½æ¸¬è©¦
        results['scale_test'] = test_scale_performance()
        
        # 2. ç·©å­˜æ•ˆæœæ¸¬è©¦
        results['cache_test'] = test_cache_effectiveness()
        
        # 3. ä½µç™¼æ€§èƒ½æ¸¬è©¦
        results['concurrent_test'] = test_concurrent_performance()
        
        # 4. ç®—æ³•å°æ¯”æ¸¬è©¦
        results['algorithm_comparison'] = test_algorithm_comparison()
        
        total_time = time.time() - start_time
        
        print(f"\nğŸ¯ åŸºæº–æ¸¬è©¦ç¸½çµ")
        print("=" * 70)
        print(f"ç¸½æ¸¬è©¦æ™‚é–“: {total_time:.2f}s")
        
        # ä¿å­˜è©³ç´°çµæœ
        benchmark_report = {
            'timestamp': time.time(),
            'total_time': total_time,
            'system_info': {
                'cpu_count': psutil.cpu_count(),
                'total_memory_gb': psutil.virtual_memory().total / 1024 / 1024 / 1024,
                'python_version': sys.version
            },
            'results': results
        }
        
        with open('benchmark_report.json', 'w', encoding='utf-8') as f:
            json.dump(benchmark_report, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“„ è©³ç´°åŸºæº–å ±å‘Šå·²ä¿å­˜åˆ°: benchmark_report.json")
        
        # é¡¯ç¤ºé—œéµæŒ‡æ¨™
        print(f"\nğŸ“ˆ é—œéµæ€§èƒ½æŒ‡æ¨™:")
        
        if 'scale_test' in results and 100 in results['scale_test']:
            scale_100 = results['scale_test'][100]
            print(f"  100å°æ•¸æ“šè™•ç†: {scale_100['total_time']:.3f}s")
            print(f"  å¹³å‡æ¯å°è™•ç†æ™‚é–“: {scale_100['avg_time_per_pair']:.4f}s")
            print(f"  ååé‡: {scale_100['throughput_pairs_per_sec']:.1f} pairs/s")
        
        if 'cache_test' in results:
            cache_result = results['cache_test']
            if 'with_cache' in cache_result and 'no_cache' in cache_result:
                speedup = cache_result['no_cache']['time'] / max(cache_result['with_cache']['time'], 0.001)
                print(f"  ç·©å­˜åŠ é€Ÿæ¯”: {speedup:.2f}x")
        
        if 'algorithm_comparison' in results:
            algo_results = results['algorithm_comparison']
            if 'basic_algorithm' in algo_results and 'optimized_algorithm' in algo_results:
                basic_throughput = algo_results['basic_algorithm']['operations'] / max(algo_results['basic_algorithm']['time'], 0.001)
                opt_throughput = algo_results['optimized_algorithm']['operations'] / max(algo_results['optimized_algorithm']['time'], 0.001)
                improvement = opt_throughput / max(basic_throughput, 0.001)
                print(f"  æ•´é«”æ€§èƒ½æå‡: {improvement:.2f}x")
        
        print(f"\nğŸ‰ æ€§èƒ½åŸºæº–æ¸¬è©¦å®Œæˆï¼")
        
    except Exception as e:
        print(f"\nâŒ åŸºæº–æ¸¬è©¦å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    run_performance_benchmark()
