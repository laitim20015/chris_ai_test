#!/usr/bin/env python3
"""
å®Œæ•´ç«¯åˆ°ç«¯æ¸¬è©¦è…³æœ¬
ä½¿ç”¨ Workflows-sample.pdf æ¸¬è©¦æ‰€æœ‰åŠŸèƒ½æµç¨‹

æ¸¬è©¦æµç¨‹ï¼š
1. æ–‡æª”è§£æ
2. åœ–ç‰‡è™•ç†  
3. åœ–æ–‡é—œè¯
4. Markdownç”Ÿæˆ
5. è¿”å›çµæœ

ç¢ºä¿ä¸ä½¿ç”¨ä»»ä½•ç°¡åŒ–ç‰ˆæœ¬ï¼ŒåŸ·è¡Œå®Œæ•´åŠŸèƒ½ã€‚
"""

import sys
import os
import asyncio
from pathlib import Path
from datetime import datetime

# è¨­ç½®é›¢ç·šæ¨¡å¼ï¼Œé¿å…Hugging Faceæ¨¡å‹ä¸‹è¼‰å•é¡Œ
os.environ['TRANSFORMERS_OFFLINE'] = '1'
os.environ['HF_DATASETS_OFFLINE'] = '1'

# æ·»åŠ srcåˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent / "src"))

from src.main import DocumentProcessor
from src.config.settings import get_settings
from src.config.logging_config import get_logger
from src.parsers.parser_factory import ParserFactory
from src.image_processing.extractor import ImageExtractor
from src.image_processing.optimizer import ImageOptimizer
from src.image_processing.storage import LocalImageStorage, StorageConfig
from src.image_processing.metadata import ImageMetadataManager
from src.association.caption_detector import CaptionDetector
from src.association.spatial_analyzer import SpatialAnalyzer
from src.association.semantic_analyzer import SemanticAnalyzer
from src.association.association_scorer import AssociationScorer
from src.association.association_optimizer import AssociationOptimizer, create_balanced_config
from src.markdown.generator import MarkdownGenerator
from src.utils.validation import validate_markdown_output


class CompleteEndToEndTest:
    """å®Œæ•´ç«¯åˆ°ç«¯æ¸¬è©¦é¡"""
    
    def __init__(self):
        """åˆå§‹åŒ–æ¸¬è©¦ç’°å¢ƒ"""
        self.logger = get_logger("end_to_end_test")
        self.settings = get_settings()
        self.test_file = Path("tests/fixtures/documents/Workflows-sample.pdf")
        
        # ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
        self.output_dir = Path("data/output")
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # çµæœçµ±è¨ˆ
        self.test_results = {
            "start_time": datetime.now(),
            "steps_completed": [],
            "errors": [],
            "warnings": [],
            "performance_metrics": {}
        }
        
        self.logger.info("ğŸ§ª åˆå§‹åŒ–å®Œæ•´ç«¯åˆ°ç«¯æ¸¬è©¦")
        self.logger.info(f"ğŸ“„ æ¸¬è©¦æ–‡ä»¶: {self.test_file}")
        self.logger.info(f"ğŸ“‚ è¼¸å‡ºç›®éŒ„: {self.output_dir}")

    async def run_complete_test(self):
        """é‹è¡Œå®Œæ•´æ¸¬è©¦"""
        try:
            self.logger.info("ğŸš€ é–‹å§‹å®Œæ•´ç«¯åˆ°ç«¯æ¸¬è©¦")
            self.logger.info("=" * 80)
            
            # æ­¥é©Ÿ1: æ–‡æª”è§£æ
            parsed_content = await self.test_document_parsing()
            
            # æ­¥é©Ÿ2: åœ–ç‰‡è™•ç†
            processed_images = await self.test_image_processing(parsed_content)
            
            # æ­¥é©Ÿ3: åœ–æ–‡é—œè¯
            associations = await self.test_image_text_association(parsed_content)
            
            # æ­¥é©Ÿ4: Markdownç”Ÿæˆ
            markdown_content = await self.test_markdown_generation(parsed_content, associations)
            
            # æ­¥é©Ÿ5: çµæœé©—è­‰å’Œè¿”å›
            final_results = await self.test_result_validation(markdown_content, parsed_content, associations)
            
            # ç”Ÿæˆæ¸¬è©¦å ±å‘Š
            await self.generate_test_report(final_results)
            
            self.logger.info("ğŸ‰ å®Œæ•´ç«¯åˆ°ç«¯æ¸¬è©¦æˆåŠŸå®Œæˆï¼")
            return final_results
            
        except Exception as e:
            self.logger.error(f"âŒ æ¸¬è©¦å¤±æ•—: {str(e)}")
            self.test_results["errors"].append(str(e))
            raise

    async def test_document_parsing(self):
        """æ­¥é©Ÿ1: å®Œæ•´æ–‡æª”è§£ææ¸¬è©¦"""
        self.logger.info("ğŸ“„ æ­¥é©Ÿ1: é–‹å§‹æ–‡æª”è§£ææ¸¬è©¦")
        start_time = datetime.now()
        
        try:
            # ä¿®å¾©è§£æå™¨å·¥å» çš„ä½¿ç”¨
            from src.parsers.parser_factory import initialize_default_parsers, get_parser_for_file
            
            # æ‰‹å‹•åˆå§‹åŒ–è§£æå™¨ï¼ˆä¿®å¾©å·¥å» å•é¡Œï¼‰
            self.logger.info("ğŸ”§ åˆå§‹åŒ–è§£æå™¨...")
            initialize_default_parsers()
            
            # è§£ææ–‡æª”
            self.logger.info("ğŸ” é–‹å§‹è§£æPDFæ–‡æª”...")
            parser = get_parser_for_file(str(self.test_file))
            
            if not parser:
                raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {self.test_file.suffix}")
            
            parsed_content = await asyncio.to_thread(
                parser.parse,
                str(self.test_file)
            )
            
            # é©—è­‰è§£æçµæœ
            self.logger.info("ğŸ“Š è§£æçµæœçµ±è¨ˆ:")
            self.logger.info(f"  â€¢ æ–‡æœ¬å¡Šæ•¸é‡: {len(parsed_content.text_blocks)}")
            self.logger.info(f"  â€¢ åœ–ç‰‡æ•¸é‡: {len(parsed_content.images)}")
            self.logger.info(f"  â€¢ è¡¨æ ¼æ•¸é‡: {len(parsed_content.tables)}")
            self.logger.info(f"  â€¢ æ–‡æª”é æ•¸: {len(set(block.page_number for block in parsed_content.text_blocks)) if parsed_content.text_blocks else 0}")
            
            # é©—è­‰å…§å®¹ä¸ç‚ºç©º
            assert len(parsed_content.text_blocks) > 0, "æ–‡æœ¬å¡Šä¸èƒ½ç‚ºç©º"
            assert parsed_content.metadata is not None, "å…ƒæ•¸æ“šä¸èƒ½ç‚ºç©º"
            
            # è¨˜éŒ„æ€§èƒ½æŒ‡æ¨™
            processing_time = (datetime.now() - start_time).total_seconds()
            self.test_results["performance_metrics"]["document_parsing"] = processing_time
            self.logger.info(f"â±ï¸ æ–‡æª”è§£æè€—æ™‚: {processing_time:.2f}ç§’")
            
            self.test_results["steps_completed"].append("document_parsing")
            self.logger.info("âœ… æ­¥é©Ÿ1: æ–‡æª”è§£æå®Œæˆ")
            
            return parsed_content
            
        except Exception as e:
            self.logger.error(f"âŒ æ–‡æª”è§£æå¤±æ•—: {str(e)}")
            self.test_results["errors"].append(f"document_parsing: {str(e)}")
            raise

    async def test_image_processing(self, parsed_content):
        """æ­¥é©Ÿ2: å®Œæ•´åœ–ç‰‡è™•ç†æ¸¬è©¦"""
        self.logger.info("ğŸ–¼ï¸ æ­¥é©Ÿ2: é–‹å§‹åœ–ç‰‡è™•ç†æ¸¬è©¦")
        start_time = datetime.now()
        
        try:
            # åˆå§‹åŒ–åœ–ç‰‡è™•ç†çµ„ä»¶
            image_extractor = ImageExtractor()
            image_optimizer = ImageOptimizer()
            
            # å‰µå»ºæœ¬åœ°å­˜å„²é…ç½®
            storage_config = StorageConfig(
                storage_type="local",
                base_path=str(self.output_dir / "images")
            )
            image_storage = LocalImageStorage(storage_config)
            metadata_manager = ImageMetadataManager()
            
            processed_images = []
            
            if not parsed_content.images:
                self.logger.warning("âš ï¸ æ²’æœ‰æ‰¾åˆ°åœ–ç‰‡ï¼Œæª¢æŸ¥PDFæ˜¯å¦åŒ…å«åœ–ç‰‡")
                self.test_results["warnings"].append("no_images_found")
                return processed_images
            
            self.logger.info(f"ğŸ” é–‹å§‹è™•ç† {len(parsed_content.images)} å¼µåœ–ç‰‡")
            
            for i, image_content in enumerate(parsed_content.images):
                self.logger.info(f"ğŸ“¸ è™•ç†åœ–ç‰‡ {i+1}/{len(parsed_content.images)}")
                
                # å°‡ImageContentè½‰æ›ç‚ºExtractedImage
                from src.image_processing.extractor import ExtractedImage
                extracted_image = ExtractedImage(
                    image_data=image_content.data,
                    format=image_content.format.value.lower(),
                    size=(image_content.width, image_content.height),
                    source_page=image_content.page_number,
                    source_position=image_content.bbox,
                    hash=image_content.checksum,
                    filename=f"image_{i+1}.{image_content.format.value.lower()}",
                    metadata={}
                )
                
                # åœ–ç‰‡å„ªåŒ–
                optimized_image = await asyncio.to_thread(
                    image_optimizer.optimize_image,
                    extracted_image
                )
                
                # ç”Ÿæˆæ–‡ä»¶å
                image_filename = f"workflows_sample_p{image_content.page_number:03d}_img{i+1:03d}.png"
                
                # å­˜å„²åœ–ç‰‡
                stored_image = await image_storage.store_image(
                    optimized_image,
                    image_filename
                )
                image_url = stored_image.url
                
                # å‰µå»ºä¸¦ä¿å­˜å…ƒæ•¸æ“š
                image_metadata = metadata_manager.create_metadata_from_extraction(
                    extracted_image,
                    self.test_file.name
                )
                
                # æ›´æ–°å­˜å„²å¾Œä¿¡æ¯
                image_metadata = metadata_manager.update_metadata_after_storage(
                    image_metadata,
                    stored_image
                )
                
                # ä¿å­˜å…ƒæ•¸æ“š
                await asyncio.to_thread(
                    metadata_manager.save_metadata,
                    image_metadata.image_id
                )
                
                # æ›´æ–°åœ–ç‰‡å…§å®¹
                image_content.url = image_url
                processed_images.append(image_content)
                
                self.logger.info(f"  âœ… åœ–ç‰‡å·²è™•ç†: {image_filename}")
                self.logger.info(f"     åŸå§‹å¤§å°: {image_metadata.original_file_size:,} bytes")
                self.logger.info(f"     å„ªåŒ–å¾Œ: {image_metadata.final_file_size:,} bytes")
                self.logger.info(f"     URL: {image_url}")
            
            # è¨˜éŒ„æ€§èƒ½æŒ‡æ¨™
            processing_time = (datetime.now() - start_time).total_seconds()
            self.test_results["performance_metrics"]["image_processing"] = processing_time
            self.logger.info(f"â±ï¸ åœ–ç‰‡è™•ç†è€—æ™‚: {processing_time:.2f}ç§’")
            
            self.test_results["steps_completed"].append("image_processing")
            self.logger.info("âœ… æ­¥é©Ÿ2: åœ–ç‰‡è™•ç†å®Œæˆ")
            
            return processed_images
            
        except Exception as e:
            self.logger.error(f"âŒ åœ–ç‰‡è™•ç†å¤±æ•—: {str(e)}")
            self.test_results["errors"].append(f"image_processing: {str(e)}")
            raise

    async def test_image_text_association(self, parsed_content):
        """æ­¥é©Ÿ3: å®Œæ•´åœ–æ–‡é—œè¯æ¸¬è©¦"""
        self.logger.info("ğŸ¯ æ­¥é©Ÿ3: é–‹å§‹åœ–æ–‡é—œè¯åˆ†ææ¸¬è©¦")
        start_time = datetime.now()
        
        try:
            # åˆå§‹åŒ–é—œè¯åˆ†æçµ„ä»¶
            caption_detector = CaptionDetector()
            spatial_analyzer = SpatialAnalyzer()
            semantic_analyzer = SemanticAnalyzer()
            association_scorer = AssociationScorer()
            
            # åˆå§‹åŒ–é—œè¯å„ªåŒ–å™¨
            association_optimizer = AssociationOptimizer(create_balanced_config())
            
            associations = []
            
            if not parsed_content.images or not parsed_content.text_blocks:
                self.logger.warning("âš ï¸ ç¼ºå°‘åœ–ç‰‡æˆ–æ–‡æœ¬å¡Šï¼Œç„¡æ³•é€²è¡Œé—œè¯åˆ†æ")
                self.test_results["warnings"].append("insufficient_content_for_association")
                return associations
            
            self.logger.info(f"ğŸ” åˆ†æ {len(parsed_content.images)} å¼µåœ–ç‰‡èˆ‡ {len(parsed_content.text_blocks)} å€‹æ–‡æœ¬å¡Šçš„é—œè¯")
            
            total_associations = 0
            
            for image in parsed_content.images:
                self.logger.info(f"ğŸ“¸ åˆ†æåœ–ç‰‡ (é é¢ {image.page_number}) çš„é—œè¯é—œä¿‚")
                
                image_associations = []
                
                for text_block in parsed_content.text_blocks:
                    # è·³éä¸åŒé é¢çš„æ–‡æœ¬ï¼ˆå¯é¸ï¼Œæ ¹æ“šéœ€æ±‚èª¿æ•´ï¼‰
                    if abs(text_block.page_number - image.page_number) > 1:
                        continue
                    
                    # ğŸ¯ ä½¿ç”¨ä¿®å¾©å¾Œçš„å®Œæ•´é—œè¯åˆ†æç³»çµ±ï¼ˆé€šéDocumentProcessorï¼‰
                    # é€™ç¢ºä¿æˆ‘å€‘ä½¿ç”¨æœ€æ–°ä¿®å¾©çš„é—œè¯ç®—æ³•å’Œæ¬Šé‡æ¨¡å‹
                    
                    # å‰µå»ºDocumentProcessorå¯¦ä¾‹ä»¥ä½¿ç”¨å®Œæ•´çš„é—œè¯åˆ†æ
                    from src.main import DocumentProcessor
                    processor = DocumentProcessor()
                    
                    # ä½¿ç”¨å®Œæ•´çš„é—œè¯åˆ†ææ–¹æ³•
                    association_result = await asyncio.to_thread(
                        processor._perform_association_analysis,
                        text_block,
                        image,
                        parsed_content  # æä¾›å®Œæ•´ä¸Šä¸‹æ–‡
                    )
                    
                    # æå–æ‰€æœ‰åˆ†æ•¸
                    final_score = association_result.get('final_score', 0.0)
                    caption_score = association_result.get('caption_score', 0.0)
                    spatial_score = association_result.get('spatial_score', 0.0)
                    semantic_score = association_result.get('semantic_score', 0.0)
                    layout_score = association_result.get('layout_score', 0.0)
                    proximity_score = association_result.get('proximity_score', 0.0)
                    
                    # ç²å–è©³ç´°åˆ†æçµæœ
                    score_details = association_result.get('details', {})
                    
                    # ä½¿ç”¨é…ç½®åŒ–é–¾å€¼åˆ¤æ–·é—œè¯
                    threshold = self.settings.association.min_association_score
                    if final_score > threshold:
                        association = {
                            "text_block_id": text_block.id,
                            "image_id": image.id,
                            "final_score": final_score,
                            "caption_score": caption_score,
                            "spatial_score": spatial_score,
                            "semantic_score": semantic_score,
                            "layout_score": layout_score,      # æ–°å¢ä½ˆå±€è©•åˆ†
                            "proximity_score": proximity_score, # æ–°å¢è·é›¢è©•åˆ†
                            "text_preview": text_block.content[:100] + "..." if len(text_block.content) > 100 else text_block.content
                        }
                        
                        image_associations.append(association)
                        total_associations += 1
                        
                        self.logger.info(f"  ğŸ¯ æ‰¾åˆ°é—œè¯ (åˆ†æ•¸: {final_score:.3f})")
                        self.logger.info(f"     Caption: {caption_score:.3f} | ç©ºé–“: {spatial_score:.3f} | èªç¾©: {semantic_score:.3f}")
                        self.logger.info(f"     ä½ˆå±€: {layout_score:.3f} | è·é›¢: {proximity_score:.3f}")
                        self.logger.info(f"     æ–‡æœ¬é è¦½: {association['text_preview']}")
                
                if image_associations:
                    # æŒ‰åˆ†æ•¸æ’åº
                    image_associations.sort(key=lambda x: x["final_score"], reverse=True)
                    associations.extend(image_associations)
                    
                    self.logger.info(f"  âœ… åœ–ç‰‡é—œè¯å®Œæˆ: æ‰¾åˆ° {len(image_associations)} å€‹é—œè¯")
                else:
                    self.logger.info(f"  âš ï¸ åœ–ç‰‡æœªæ‰¾åˆ°é«˜è³ªé‡é—œè¯")
            
            # ğŸ”§ é—œè¯å„ªåŒ– - å»é‡ã€éæ¿¾å’Œè³ªé‡æå‡
            self.logger.info(f"ğŸ”§ é–‹å§‹é—œè¯å„ªåŒ– - åŸå§‹é—œè¯æ•¸: {len(associations)}")
            optimized_associations = await asyncio.to_thread(
                association_optimizer.optimize_associations,
                associations,
                parsed_content.images,
                parsed_content.text_blocks
            )
            self.logger.info(f"é—œè¯å„ªåŒ–å®Œæˆ - å„ªåŒ–å¾Œé—œè¯æ•¸: {len(optimized_associations)}")
            reduction_rate = ((len(associations) - len(optimized_associations)) / len(associations) * 100) if associations else 0
            self.logger.info(f"é—œè¯æ¸›å°‘ç‡: {reduction_rate:.1f}%")
            
            # ä½¿ç”¨å„ªåŒ–å¾Œçš„é—œè¯
            associations = optimized_associations
            
            # è¨˜éŒ„æ€§èƒ½æŒ‡æ¨™
            processing_time = (datetime.now() - start_time).total_seconds()
            self.test_results["performance_metrics"]["association_analysis"] = processing_time
            self.logger.info(f"â±ï¸ é—œè¯åˆ†æè€—æ™‚: {processing_time:.2f}ç§’")
            self.logger.info(f"ğŸ“Š ç¸½é—œè¯æ•¸: {total_associations}")
            
            self.test_results["steps_completed"].append("association_analysis")
            self.logger.info("âœ… æ­¥é©Ÿ3: åœ–æ–‡é—œè¯åˆ†æå®Œæˆ")
            
            return associations
            
        except Exception as e:
            self.logger.error(f"âŒ åœ–æ–‡é—œè¯åˆ†æå¤±æ•—: {str(e)}")
            self.test_results["errors"].append(f"association_analysis: {str(e)}")
            raise

    async def test_markdown_generation(self, parsed_content, associations):
        """æ­¥é©Ÿ4: å®Œæ•´Markdownç”Ÿæˆæ¸¬è©¦"""
        self.logger.info("ğŸ“ æ­¥é©Ÿ4: é–‹å§‹Markdownç”Ÿæˆæ¸¬è©¦")
        start_time = datetime.now()
        
        try:
            # åˆå§‹åŒ–Markdownç”Ÿæˆå™¨
            markdown_generator = MarkdownGenerator()
            
            self.logger.info("ğŸ”¨ ç”Ÿæˆçµæ§‹åŒ–Markdownå…§å®¹...")
            
            # æ§‹å»ºé—œè¯æ˜ å°„
            association_map = {}
            for assoc in associations:
                text_id = assoc["text_block_id"]
                if text_id not in association_map:
                    association_map[text_id] = []
                association_map[text_id].append(assoc)
            
            # ç”ŸæˆMarkdown
            markdown_content = await asyncio.to_thread(
                markdown_generator.generate,
                parsed_content,
                associations,
                template_name="enhanced.md.j2",
                include_metadata=True
            )
            
            # é©—è­‰Markdownæ ¼å¼
            is_valid = validate_markdown_output(markdown_content)
            if not is_valid:
                self.logger.warning("âš ï¸ Markdownæ ¼å¼é©—è­‰å¤±æ•—")
                self.test_results["warnings"].append("markdown_validation_failed")
            
            # ä¿å­˜Markdownæ–‡ä»¶
            output_file = self.output_dir / "workflows_sample_complete.md"
            with open(output_file, "w", encoding="utf-8") as f:
                f.write(markdown_content)
            
            # çµ±è¨ˆä¿¡æ¯
            lines_count = len(markdown_content.split('\n'))
            chars_count = len(markdown_content)
            images_in_markdown = markdown_content.count('![')
            
            self.logger.info("ğŸ“Š Markdownç”Ÿæˆçµ±è¨ˆ:")
            self.logger.info(f"  â€¢ ç¸½è¡Œæ•¸: {lines_count}")
            self.logger.info(f"  â€¢ ç¸½å­—ç¬¦æ•¸: {chars_count:,}")
            self.logger.info(f"  â€¢ åµŒå…¥åœ–ç‰‡æ•¸: {images_in_markdown}")
            self.logger.info(f"  â€¢ è¼¸å‡ºæ–‡ä»¶: {output_file}")
            
            # è¨˜éŒ„æ€§èƒ½æŒ‡æ¨™
            processing_time = (datetime.now() - start_time).total_seconds()
            self.test_results["performance_metrics"]["markdown_generation"] = processing_time
            self.logger.info(f"â±ï¸ Markdownç”Ÿæˆè€—æ™‚: {processing_time:.2f}ç§’")
            
            self.test_results["steps_completed"].append("markdown_generation")
            self.logger.info("âœ… æ­¥é©Ÿ4: Markdownç”Ÿæˆå®Œæˆ")
            
            return markdown_content
            
        except Exception as e:
            self.logger.error(f"âŒ Markdownç”Ÿæˆå¤±æ•—: {str(e)}")
            self.test_results["errors"].append(f"markdown_generation: {str(e)}")
            raise

    async def test_result_validation(self, markdown_content, parsed_content, associations):
        """æ­¥é©Ÿ5: çµæœé©—è­‰å’Œè¿”å›"""
        self.logger.info("ğŸ” æ­¥é©Ÿ5: é–‹å§‹çµæœé©—è­‰")
        start_time = datetime.now()
        
        try:
            # æ§‹å»ºå®Œæ•´çµæœ
            final_results = {
                "test_metadata": {
                    "test_file": str(self.test_file),
                    "test_date": datetime.now().isoformat(),
                    "total_processing_time": (datetime.now() - self.test_results["start_time"]).total_seconds()
                },
                "document_info": {
                    "filename": self.test_file.name,
                    "file_size": self.test_file.stat().st_size,
                    "text_blocks_count": len(parsed_content.text_blocks),
                    "images_count": len(parsed_content.images),
                    "tables_count": len(parsed_content.tables)
                },
                "processing_results": {
                    "associations_count": len(associations),
                    "markdown_length": len(markdown_content),
                    "markdown_lines": len(markdown_content.split('\n')),
                    "images_in_markdown": markdown_content.count('![')
                },
                "performance_metrics": self.test_results["performance_metrics"],
                "quality_metrics": {
                    "steps_completed": len(self.test_results["steps_completed"]),
                    "total_steps": 5,
                    "success_rate": len(self.test_results["steps_completed"]) / 5 * 100,
                    "errors_count": len(self.test_results["errors"]),
                    "warnings_count": len(self.test_results["warnings"])
                },
                "outputs": {
                    "markdown_content": markdown_content[:1000] + "..." if len(markdown_content) > 1000 else markdown_content,
                    "sample_associations": associations[:3] if associations else []
                }
            }
            
            # é©—è­‰å®Œæ•´æ€§
            validation_checks = {
                "document_parsed": len(parsed_content.text_blocks) > 0,
                "images_processed": len(parsed_content.images) >= 0,  # å…è¨±ç„¡åœ–ç‰‡
                "associations_analyzed": True,  # å³ä½¿æ²’æœ‰é—œè¯ä¹Ÿç®—å®Œæˆ
                "markdown_generated": len(markdown_content) > 0,
                "no_critical_errors": len(self.test_results["errors"]) == 0
            }
            
            final_results["validation_checks"] = validation_checks
            all_checks_passed = all(validation_checks.values())
            
            self.logger.info("ğŸ“‹ çµæœé©—è­‰æ‘˜è¦:")
            for check, passed in validation_checks.items():
                status = "âœ…" if passed else "âŒ"
                self.logger.info(f"  {status} {check}")
            
            if all_checks_passed:
                self.logger.info("ğŸ‰ æ‰€æœ‰é©—è­‰æª¢æŸ¥é€šéï¼")
            else:
                self.logger.warning("âš ï¸ éƒ¨åˆ†é©—è­‰æª¢æŸ¥æœªé€šé")
            
            # è¨˜éŒ„æ€§èƒ½æŒ‡æ¨™
            processing_time = (datetime.now() - start_time).total_seconds()
            self.test_results["performance_metrics"]["result_validation"] = processing_time
            
            self.test_results["steps_completed"].append("result_validation")
            
            # é‡æ–°è¨ˆç®—æˆåŠŸç‡ï¼ˆä¿®å¾©æ™‚åºå•é¡Œï¼‰
            final_results["quality_metrics"]["steps_completed"] = len(self.test_results["steps_completed"])
            final_results["quality_metrics"]["success_rate"] = len(self.test_results["steps_completed"]) / 5 * 100
            
            self.logger.info("âœ… æ­¥é©Ÿ5: çµæœé©—è­‰å®Œæˆ")
            
            return final_results
            
        except Exception as e:
            self.logger.error(f"âŒ çµæœé©—è­‰å¤±æ•—: {str(e)}")
            self.test_results["errors"].append(f"result_validation: {str(e)}")
            raise

    async def generate_test_report(self, final_results):
        """ç”Ÿæˆè©³ç´°æ¸¬è©¦å ±å‘Š"""
        self.logger.info("ğŸ“Š ç”Ÿæˆæ¸¬è©¦å ±å‘Š...")
        
        report_content = f"""# å®Œæ•´ç«¯åˆ°ç«¯æ¸¬è©¦å ±å‘Š

## æ¸¬è©¦æ¦‚è¿°
- **æ¸¬è©¦æ–‡ä»¶**: {final_results['test_metadata']['test_file']}
- **æ¸¬è©¦æ™‚é–“**: {final_results['test_metadata']['test_date']}
- **ç¸½è™•ç†æ™‚é–“**: {final_results['test_metadata']['total_processing_time']:.2f}ç§’

## æ–‡æª”ä¿¡æ¯
- **æ–‡ä»¶å**: {final_results['document_info']['filename']}
- **æ–‡ä»¶å¤§å°**: {final_results['document_info']['file_size']:,} bytes
- **æ–‡æœ¬å¡Šæ•¸**: {final_results['document_info']['text_blocks_count']}
- **åœ–ç‰‡æ•¸**: {final_results['document_info']['images_count']}
- **è¡¨æ ¼æ•¸**: {final_results['document_info']['tables_count']}

## è™•ç†çµæœ
- **é—œè¯æ•¸**: {final_results['processing_results']['associations_count']}
- **Markdowné•·åº¦**: {final_results['processing_results']['markdown_length']:,} å­—ç¬¦
- **Markdownè¡Œæ•¸**: {final_results['processing_results']['markdown_lines']}
- **åµŒå…¥åœ–ç‰‡æ•¸**: {final_results['processing_results']['images_in_markdown']}

## æ€§èƒ½æŒ‡æ¨™
"""
        
        for step, time_taken in final_results['performance_metrics'].items():
            report_content += f"- **{step}**: {time_taken:.2f}ç§’\n"
        
        report_content += f"""
## è³ªé‡æŒ‡æ¨™
- **å®Œæˆæ­¥é©Ÿ**: {final_results['quality_metrics']['steps_completed']}/{final_results['quality_metrics']['total_steps']}
- **æˆåŠŸç‡**: {final_results['quality_metrics']['success_rate']:.1f}%
- **éŒ¯èª¤æ•¸**: {final_results['quality_metrics']['errors_count']}
- **è­¦å‘Šæ•¸**: {final_results['quality_metrics']['warnings_count']}

## é©—è­‰æª¢æŸ¥
"""
        
        for check, passed in final_results['validation_checks'].items():
            status = "âœ… é€šé" if passed else "âŒ å¤±æ•—"
            report_content += f"- **{check}**: {status}\n"
        
        if final_results['outputs']['sample_associations']:
            report_content += "\n## æ¨£æœ¬é—œè¯\n"
            for i, assoc in enumerate(final_results['outputs']['sample_associations'], 1):
                report_content += f"### é—œè¯ {i}\n"
                report_content += f"- **é—œè¯åˆ†æ•¸**: {assoc['final_score']:.3f}\n"
                report_content += f"- **Captionåˆ†æ•¸**: {assoc['caption_score']:.3f}\n"
                report_content += f"- **ç©ºé–“åˆ†æ•¸**: {assoc['spatial_score']:.3f}\n"
                report_content += f"- **èªç¾©åˆ†æ•¸**: {assoc['semantic_score']:.3f}\n"
                report_content += f"- **æ–‡æœ¬é è¦½**: {assoc['text_preview']}\n\n"
        
        # ä¿å­˜å ±å‘Š
        report_file = self.output_dir / "test_report.md"
        with open(report_file, "w", encoding="utf-8") as f:
            f.write(report_content)
        
        self.logger.info(f"ğŸ“Š æ¸¬è©¦å ±å‘Šå·²ä¿å­˜: {report_file}")


async def main():
    """ä¸»æ¸¬è©¦å‡½æ•¸"""
    test = CompleteEndToEndTest()
    
    try:
        # é‹è¡Œå®Œæ•´æ¸¬è©¦
        results = await test.run_complete_test()
        
        print("\n" + "="*80)
        print("ğŸ‰ å®Œæ•´ç«¯åˆ°ç«¯æ¸¬è©¦æˆåŠŸå®Œæˆï¼")
        print("="*80)
        print(f"ğŸ“Š ç¸½è™•ç†æ™‚é–“: {results['test_metadata']['total_processing_time']:.2f}ç§’")
        print(f"âœ… æˆåŠŸç‡: {results['quality_metrics']['success_rate']:.1f}%")
        print(f"ğŸ“„ æ–‡æœ¬å¡Š: {results['document_info']['text_blocks_count']}")
        print(f"ğŸ–¼ï¸ åœ–ç‰‡: {results['document_info']['images_count']}")
        print(f"ğŸ¯ é—œè¯: {results['processing_results']['associations_count']}")
        print(f"ğŸ“ Markdown: {results['processing_results']['markdown_length']:,} å­—ç¬¦")
        print("="*80)
        
        return 0
        
    except Exception as e:
        print(f"\nâŒ æ¸¬è©¦å¤±æ•—: {str(e)}")
        return 1


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)
