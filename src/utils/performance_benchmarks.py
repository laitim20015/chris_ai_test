"""
æ€§èƒ½åŸºæº–æ¸¬è©¦æ¨¡çµ„
Performance Benchmarks Module

æä¾›å…¨é¢çš„æ€§èƒ½æ¸¬è©¦ã€åŸºæº–æ¯”è¼ƒå’Œæ€§èƒ½ç›£æŽ§åŠŸèƒ½ã€‚
æ¸¬è©¦è™•ç†é€Ÿåº¦ã€å…§å­˜ä½¿ç”¨ã€æº–ç¢ºæ€§ç­‰é—œéµæŒ‡æ¨™ã€‚
"""

import time
import psutil
import gc
import asyncio
import json
import platform
from typing import Dict, List, Any, Optional, Callable, Union
from dataclasses import dataclass, field, asdict
from pathlib import Path
from datetime import datetime
from statistics import mean, stdev
import tracemalloc
from contextlib import contextmanager

from src.config.logging_config import get_logger

logger = get_logger("performance_benchmarks")

@dataclass
class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ¨™æ•¸æ“šçµæ§‹"""
    operation_name: str                           # æ“ä½œåç¨±
    execution_time: float                         # åŸ·è¡Œæ™‚é–“ï¼ˆç§’ï¼‰
    memory_peak: float = 0.0                     # å³°å€¼å…§å­˜ä½¿ç”¨ï¼ˆMBï¼‰
    memory_current: float = 0.0                  # ç•¶å‰å…§å­˜ä½¿ç”¨ï¼ˆMBï¼‰
    cpu_percent: float = 0.0                     # CPUä½¿ç”¨çŽ‡
    success: bool = True                          # æ˜¯å¦æˆåŠŸ
    error_message: str = ""                       # éŒ¯èª¤ä¿¡æ¯
    input_size: int = 0                          # è¼¸å…¥å¤§å°
    output_size: int = 0                         # è¼¸å‡ºå¤§å°
    throughput: float = 0.0                      # åžåé‡ï¼ˆæ“ä½œ/ç§’ï¼‰
    additional_metrics: Dict[str, Any] = field(default_factory=dict)
    timestamp: datetime = field(default_factory=datetime.now)

@dataclass
class BenchmarkSuite:
    """åŸºæº–æ¸¬è©¦å¥—ä»¶"""
    name: str                                     # å¥—ä»¶åç¨±
    description: str                              # æè¿°
    metrics: List[PerformanceMetrics] = field(default_factory=list)
    baseline_metrics: Optional[PerformanceMetrics] = None
    created_at: datetime = field(default_factory=datetime.now)

class PerformanceProfiler:
    """æ€§èƒ½åˆ†æžå™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–æ€§èƒ½åˆ†æžå™¨"""
        self.logger = logger
        self.current_metrics: Optional[PerformanceMetrics] = None
        self.suites: Dict[str, BenchmarkSuite] = {}
        
        # ç³»çµ±ä¿¡æ¯
        self.system_info = {
            "cpu_count": psutil.cpu_count(),
            "memory_total": psutil.virtual_memory().total / (1024**3),  # GB
            "platform": platform.platform()
        }
        
        self.logger.info(f"æ€§èƒ½åˆ†æžå™¨åˆå§‹åŒ–å®Œæˆ - CPU: {self.system_info['cpu_count']}æ ¸, å…§å­˜: {self.system_info['memory_total']:.1f}GB")
    
    @contextmanager
    def profile_operation(self, operation_name: str, input_size: int = 0):
        """
        æ€§èƒ½åˆ†æžä¸Šä¸‹æ–‡ç®¡ç†å™¨
        
        Args:
            operation_name: æ“ä½œåç¨±
            input_size: è¼¸å…¥å¤§å°
            
        Yields:
            PerformanceMetrics: æ€§èƒ½æŒ‡æ¨™å°è±¡
        """
        # é–‹å§‹å…§å­˜è·Ÿè¸ª
        tracemalloc.start()
        
        # è¨˜éŒ„é–‹å§‹ç‹€æ…‹
        start_time = time.perf_counter()
        start_memory = psutil.virtual_memory().used / (1024**2)  # MB
        process = psutil.Process()
        start_cpu = process.cpu_percent()
        
        # åˆå§‹åŒ–æŒ‡æ¨™å°è±¡
        metrics = PerformanceMetrics(
            operation_name=operation_name,
            execution_time=0.0,
            input_size=input_size
        )
        
        try:
            # åŸ·è¡Œæ“ä½œ
            yield metrics
            
            # è¨˜éŒ„æˆåŠŸ
            metrics.success = True
            
        except Exception as e:
            # è¨˜éŒ„éŒ¯èª¤
            metrics.success = False
            metrics.error_message = str(e)
            self.logger.error(f"æ€§èƒ½æ¸¬è©¦ä¸­ç™¼ç”ŸéŒ¯èª¤ {operation_name}: {e}")
            
        finally:
            # è¨ˆç®—æ€§èƒ½æŒ‡æ¨™
            end_time = time.perf_counter()
            end_memory = psutil.virtual_memory().used / (1024**2)  # MB
            end_cpu = process.cpu_percent()
            
            # ç²å–å…§å­˜å³°å€¼
            current, peak = tracemalloc.get_traced_memory()
            tracemalloc.stop()
            
            # æ›´æ–°æŒ‡æ¨™
            metrics.execution_time = end_time - start_time
            metrics.memory_current = end_memory - start_memory
            metrics.memory_peak = peak / (1024**2)  # è½‰æ›ç‚ºMB
            metrics.cpu_percent = (start_cpu + end_cpu) / 2
            
            # è¨ˆç®—åžåé‡
            if metrics.execution_time > 0:
                metrics.throughput = 1.0 / metrics.execution_time
            
            self.current_metrics = metrics
            self.logger.debug(f"æ€§èƒ½åˆ†æžå®Œæˆ: {operation_name} - {metrics.execution_time:.3f}s")
    
    def benchmark_function(self, 
                          func: Callable,
                          args: tuple = (),
                          kwargs: dict = None,
                          iterations: int = 1,
                          warmup: int = 0) -> List[PerformanceMetrics]:
        """
        å°å‡½æ•¸é€²è¡ŒåŸºæº–æ¸¬è©¦
        
        Args:
            func: è¦æ¸¬è©¦çš„å‡½æ•¸
            args: å‡½æ•¸åƒæ•¸
            kwargs: å‡½æ•¸é—œéµå­—åƒæ•¸
            iterations: æ¸¬è©¦è¿­ä»£æ¬¡æ•¸
            warmup: é ç†±æ¬¡æ•¸
            
        Returns:
            List[PerformanceMetrics]: æ€§èƒ½æŒ‡æ¨™åˆ—è¡¨
        """
        kwargs = kwargs or {}
        function_name = getattr(func, '__name__', str(func))
        
        self.logger.info(f"é–‹å§‹åŸºæº–æ¸¬è©¦: {function_name} ({iterations}æ¬¡è¿­ä»£ï¼Œ{warmup}æ¬¡é ç†±)")
        
        # é ç†±
        for i in range(warmup):
            try:
                func(*args, **kwargs)
            except Exception as e:
                self.logger.warning(f"é ç†± {i+1} å¤±æ•—: {e}")
        
        # å¯¦éš›æ¸¬è©¦
        results = []
        for i in range(iterations):
            with self.profile_operation(f"{function_name}_iter_{i+1}") as metrics:
                try:
                    result = func(*args, **kwargs)
                    
                    # å˜—è©¦ç²å–è¼¸å‡ºå¤§å°
                    if hasattr(result, '__len__'):
                        metrics.output_size = len(result)
                    elif isinstance(result, str):
                        metrics.output_size = len(result.encode('utf-8'))
                    
                except Exception:
                    raise  # é‡æ–°æ‹‹å‡ºç•°å¸¸ä»¥ä¾¿ä¸Šä¸‹æ–‡ç®¡ç†å™¨è™•ç†
                
            results.append(metrics)
        
        # è¨ˆç®—çµ±è¨ˆä¿¡æ¯
        self._log_benchmark_summary(function_name, results)
        
        return results
    
    async def benchmark_async_function(self,
                                     func: Callable,
                                     args: tuple = (),
                                     kwargs: dict = None,
                                     iterations: int = 1,
                                     warmup: int = 0) -> List[PerformanceMetrics]:
        """
        å°ç•°æ­¥å‡½æ•¸é€²è¡ŒåŸºæº–æ¸¬è©¦
        
        Args:
            func: è¦æ¸¬è©¦çš„ç•°æ­¥å‡½æ•¸
            args: å‡½æ•¸åƒæ•¸
            kwargs: å‡½æ•¸é—œéµå­—åƒæ•¸
            iterations: æ¸¬è©¦è¿­ä»£æ¬¡æ•¸
            warmup: é ç†±æ¬¡æ•¸
            
        Returns:
            List[PerformanceMetrics]: æ€§èƒ½æŒ‡æ¨™åˆ—è¡¨
        """
        kwargs = kwargs or {}
        function_name = getattr(func, '__name__', str(func))
        
        self.logger.info(f"é–‹å§‹ç•°æ­¥åŸºæº–æ¸¬è©¦: {function_name} ({iterations}æ¬¡è¿­ä»£ï¼Œ{warmup}æ¬¡é ç†±)")
        
        # é ç†±
        for i in range(warmup):
            try:
                await func(*args, **kwargs)
            except Exception as e:
                self.logger.warning(f"é ç†± {i+1} å¤±æ•—: {e}")
        
        # å¯¦éš›æ¸¬è©¦
        results = []
        for i in range(iterations):
            with self.profile_operation(f"{function_name}_async_iter_{i+1}") as metrics:
                try:
                    result = await func(*args, **kwargs)
                    
                    # å˜—è©¦ç²å–è¼¸å‡ºå¤§å°
                    if hasattr(result, '__len__'):
                        metrics.output_size = len(result)
                    elif isinstance(result, str):
                        metrics.output_size = len(result.encode('utf-8'))
                    
                except Exception:
                    raise  # é‡æ–°æ‹‹å‡ºç•°å¸¸ä»¥ä¾¿ä¸Šä¸‹æ–‡ç®¡ç†å™¨è™•ç†
                
            results.append(metrics)
        
        # è¨ˆç®—çµ±è¨ˆä¿¡æ¯
        self._log_benchmark_summary(function_name + "_async", results)
        
        return results
    
    def _log_benchmark_summary(self, function_name: str, results: List[PerformanceMetrics]):
        """è¨˜éŒ„åŸºæº–æ¸¬è©¦æ‘˜è¦"""
        successful_results = [r for r in results if r.success]
        
        if not successful_results:
            self.logger.error(f"åŸºæº–æ¸¬è©¦å…¨éƒ¨å¤±æ•—: {function_name}")
            return
        
        times = [r.execution_time for r in successful_results]
        memories = [r.memory_peak for r in successful_results]
        
        avg_time = mean(times)
        std_time = stdev(times) if len(times) > 1 else 0
        avg_memory = mean(memories)
        success_rate = len(successful_results) / len(results) * 100
        
        self.logger.info(f"ðŸ“Š åŸºæº–æ¸¬è©¦æ‘˜è¦: {function_name}")
        self.logger.info(f"  â±ï¸  å¹³å‡æ™‚é–“: {avg_time:.3f}s (Â±{std_time:.3f}s)")
        self.logger.info(f"  ðŸ§  å¹³å‡å…§å­˜: {avg_memory:.2f}MB")
        self.logger.info(f"  âœ… æˆåŠŸçŽ‡: {success_rate:.1f}%")
        self.logger.info(f"  ðŸ”„ åžåé‡: {1/avg_time:.2f} ops/sec")
    
    def create_benchmark_suite(self, name: str, description: str) -> BenchmarkSuite:
        """
        å‰µå»ºåŸºæº–æ¸¬è©¦å¥—ä»¶
        
        Args:
            name: å¥—ä»¶åç¨±
            description: æè¿°
            
        Returns:
            BenchmarkSuite: åŸºæº–æ¸¬è©¦å¥—ä»¶
        """
        suite = BenchmarkSuite(name=name, description=description)
        self.suites[name] = suite
        
        self.logger.info(f"å‰µå»ºåŸºæº–æ¸¬è©¦å¥—ä»¶: {name}")
        
        return suite
    
    def add_to_suite(self, suite_name: str, metrics: Union[PerformanceMetrics, List[PerformanceMetrics]]):
        """
        æ·»åŠ æŒ‡æ¨™åˆ°æ¸¬è©¦å¥—ä»¶
        
        Args:
            suite_name: å¥—ä»¶åç¨±
            metrics: æ€§èƒ½æŒ‡æ¨™
        """
        if suite_name not in self.suites:
            raise ValueError(f"æ¸¬è©¦å¥—ä»¶ä¸å­˜åœ¨: {suite_name}")
        
        suite = self.suites[suite_name]
        
        if isinstance(metrics, list):
            suite.metrics.extend(metrics)
        else:
            suite.metrics.append(metrics)
    
    def compare_with_baseline(self, suite_name: str, baseline_metrics: PerformanceMetrics) -> Dict[str, float]:
        """
        èˆ‡åŸºæº–ç·šæ¯”è¼ƒ
        
        Args:
            suite_name: å¥—ä»¶åç¨±
            baseline_metrics: åŸºæº–ç·šæŒ‡æ¨™
            
        Returns:
            Dict[str, float]: æ¯”è¼ƒçµæžœï¼ˆæ¯”çŽ‡ï¼‰
        """
        if suite_name not in self.suites:
            raise ValueError(f"æ¸¬è©¦å¥—ä»¶ä¸å­˜åœ¨: {suite_name}")
        
        suite = self.suites[suite_name]
        suite.baseline_metrics = baseline_metrics
        
        successful_metrics = [m for m in suite.metrics if m.success]
        if not successful_metrics:
            return {}
        
        # è¨ˆç®—å¹³å‡å€¼
        avg_time = mean([m.execution_time for m in successful_metrics])
        avg_memory = mean([m.memory_peak for m in successful_metrics])
        
        # è¨ˆç®—æ¯”çŽ‡ï¼ˆç•¶å‰/åŸºæº–ç·šï¼‰
        comparison = {
            "time_ratio": avg_time / baseline_metrics.execution_time,
            "memory_ratio": avg_memory / baseline_metrics.memory_peak,
            "throughput_ratio": (1/avg_time) / baseline_metrics.throughput
        }
        
        self.logger.info(f"ðŸ“ˆ åŸºæº–ç·šæ¯”è¼ƒ ({suite_name}):")
        self.logger.info(f"  â±ï¸  æ™‚é–“æ¯”çŽ‡: {comparison['time_ratio']:.2f}x")
        self.logger.info(f"  ðŸ§  å…§å­˜æ¯”çŽ‡: {comparison['memory_ratio']:.2f}x")
        self.logger.info(f"  ðŸ”„ åžåé‡æ¯”çŽ‡: {comparison['throughput_ratio']:.2f}x")
        
        return comparison
    
    def export_results(self, file_path: Union[str, Path]) -> bool:
        """
        å°Žå‡ºæ¸¬è©¦çµæžœ
        
        Args:
            file_path: è¼¸å‡ºæ–‡ä»¶è·¯å¾‘
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        try:
            # æº–å‚™å°Žå‡ºæ•¸æ“š
            export_data = {
                "system_info": self.system_info,
                "export_time": datetime.now().isoformat(),
                "suites": {}
            }
            
            for name, suite in self.suites.items():
                export_data["suites"][name] = {
                    "name": suite.name,
                    "description": suite.description,
                    "created_at": suite.created_at.isoformat(),
                    "metrics": [asdict(m) for m in suite.metrics],
                    "baseline_metrics": asdict(suite.baseline_metrics) if suite.baseline_metrics else None
                }
            
            # å¯«å…¥æ–‡ä»¶
            file_path = Path(file_path)
            file_path.parent.mkdir(parents=True, exist_ok=True)
            
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(export_data, f, indent=2, ensure_ascii=False, default=str)
            
            self.logger.info(f"æ€§èƒ½æ¸¬è©¦çµæžœå·²å°Žå‡º: {file_path}")
            return True
            
        except Exception as e:
            self.logger.error(f"å°Žå‡ºçµæžœå¤±æ•—: {e}")
            return False
    
    def generate_report(self, suite_name: str) -> Dict[str, Any]:
        """
        ç”Ÿæˆæ€§èƒ½å ±å‘Š
        
        Args:
            suite_name: å¥—ä»¶åç¨±
            
        Returns:
            Dict[str, Any]: æ€§èƒ½å ±å‘Š
        """
        if suite_name not in self.suites:
            raise ValueError(f"æ¸¬è©¦å¥—ä»¶ä¸å­˜åœ¨: {suite_name}")
        
        suite = self.suites[suite_name]
        successful_metrics = [m for m in suite.metrics if m.success]
        failed_metrics = [m for m in suite.metrics if not m.success]
        
        if not successful_metrics:
            return {"error": "æ²’æœ‰æˆåŠŸçš„æ¸¬è©¦çµæžœ"}
        
        # çµ±è¨ˆæ•¸æ“š
        times = [m.execution_time for m in successful_metrics]
        memories = [m.memory_peak for m in successful_metrics]
        
        report = {
            "suite_name": suite.name,
            "description": suite.description,
            "total_tests": len(suite.metrics),
            "successful_tests": len(successful_metrics),
            "failed_tests": len(failed_metrics),
            "success_rate": len(successful_metrics) / len(suite.metrics) * 100,
            "performance_stats": {
                "execution_time": {
                    "min": min(times),
                    "max": max(times),
                    "avg": mean(times),
                    "std": stdev(times) if len(times) > 1 else 0
                },
                "memory_usage": {
                    "min": min(memories),
                    "max": max(memories),
                    "avg": mean(memories),
                    "std": stdev(memories) if len(memories) > 1 else 0
                },
                "throughput": {
                    "avg": 1 / mean(times),
                    "peak": 1 / min(times)
                }
            },
            "system_info": self.system_info,
            "generated_at": datetime.now().isoformat()
        }
        
        # å¦‚æžœæœ‰åŸºæº–ç·šï¼Œæ·»åŠ æ¯”è¼ƒ
        if suite.baseline_metrics:
            comparison = self.compare_with_baseline(suite_name, suite.baseline_metrics)
            report["baseline_comparison"] = comparison
        
        return report

# å…¨å±€æ€§èƒ½åˆ†æžå™¨å¯¦ä¾‹
_global_profiler = PerformanceProfiler()

def get_profiler() -> PerformanceProfiler:
    """ç²å–å…¨å±€æ€§èƒ½åˆ†æžå™¨"""
    return _global_profiler

def profile(operation_name: str = None, input_size: int = 0):
    """
    æ€§èƒ½åˆ†æžè£é£¾å™¨
    
    Args:
        operation_name: æ“ä½œåç¨±
        input_size: è¼¸å…¥å¤§å°
        
    Returns:
        è£é£¾å¾Œçš„å‡½æ•¸
    """
    def decorator(func: Callable) -> Callable:
        actual_name = operation_name or getattr(func, '__name__', str(func))
        
        if asyncio.iscoroutinefunction(func):
            async def async_wrapper(*args, **kwargs):
                with get_profiler().profile_operation(actual_name, input_size) as metrics:
                    return await func(*args, **kwargs)
            return async_wrapper
        else:
            def sync_wrapper(*args, **kwargs):
                with get_profiler().profile_operation(actual_name, input_size) as metrics:
                    return func(*args, **kwargs)
            return sync_wrapper
    
    return decorator
