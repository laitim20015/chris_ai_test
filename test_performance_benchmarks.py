#!/usr/bin/env python3
"""
æ€§èƒ½åŸºæº–æ¸¬è©¦æ¼”ç¤º
Performance Benchmarks Demo

æ¼”ç¤ºæ€§èƒ½æ¸¬è©¦ç³»çµ±çš„åŠŸèƒ½ï¼ŒåŒ…æ‹¬åŸºæº–æ¸¬è©¦ã€æ€§èƒ½åˆ†æå’Œå ±å‘Šç”Ÿæˆã€‚
"""

import sys
import time
import asyncio
from pathlib import Path

# æ·»åŠ srcåˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent / "src"))

def test_performance_benchmarks():
    """æ¸¬è©¦æ€§èƒ½åŸºæº–æ¸¬è©¦ç³»çµ±"""
    
    print("ğŸ§ª æ€§èƒ½åŸºæº–æ¸¬è©¦ç³»çµ±æ¼”ç¤º")
    print("=" * 50)
    
    try:
        from utils.performance_benchmarks import (
            PerformanceProfiler,
            PerformanceMetrics,
            get_profiler,
            profile
        )
        
        # åˆå§‹åŒ–æ€§èƒ½åˆ†æå™¨
        profiler = PerformanceProfiler()
        
        # å‰µå»ºæ¸¬è©¦å¥—ä»¶
        print("ğŸ“Š å‰µå»ºæ€§èƒ½æ¸¬è©¦å¥—ä»¶...")
        suite = profiler.create_benchmark_suite(
            "document_processing_benchmark",
            "æ–‡æª”è™•ç†æ€§èƒ½åŸºæº–æ¸¬è©¦"
        )
        
        # æ¸¬è©¦åŒæ­¥å‡½æ•¸æ€§èƒ½
        print("\nâ±ï¸ æ¸¬è©¦åŒæ­¥å‡½æ•¸æ€§èƒ½...")
        
        def slow_text_processing(text_length: int = 1000):
            """æ¨¡æ“¬æ…¢é€Ÿæ–‡æœ¬è™•ç†"""
            text = "æ¸¬è©¦æ–‡æœ¬ " * text_length
            
            # æ¨¡æ“¬ä¸€äº›è™•ç†æ™‚é–“
            processed_words = []
            for word in text.split():
                processed_words.append(word.upper())
                if len(processed_words) % 100 == 0:
                    time.sleep(0.001)  # æ¨¡æ“¬è™•ç†å»¶é²
            
            return " ".join(processed_words)
        
        # åŸºæº–æ¸¬è©¦
        sync_results = profiler.benchmark_function(
            slow_text_processing,
            kwargs={"text_length": 500},
            iterations=3,
            warmup=1
        )
        
        profiler.add_to_suite("document_processing_benchmark", sync_results)
        
        # æ¸¬è©¦ç•°æ­¥å‡½æ•¸æ€§èƒ½
        print("\nâš¡ æ¸¬è©¦ç•°æ­¥å‡½æ•¸æ€§èƒ½...")
        
        async def async_network_simulation(requests: int = 10):
            """æ¨¡æ“¬ç•°æ­¥ç¶²çµ¡è«‹æ±‚"""
            results = []
            
            for i in range(requests):
                # æ¨¡æ“¬ç•°æ­¥I/Oå»¶é²
                await asyncio.sleep(0.01)
                results.append(f"éŸ¿æ‡‰_{i+1}")
            
            return results
        
        # ç•°æ­¥åŸºæº–æ¸¬è©¦
        async_results = asyncio.run(
            profiler.benchmark_async_function(
                async_network_simulation,
                kwargs={"requests": 5},
                iterations=3,
                warmup=1
            )
        )
        
        profiler.add_to_suite("document_processing_benchmark", async_results)
        
        # ä½¿ç”¨è£é£¾å™¨é€²è¡Œæ€§èƒ½åˆ†æ
        print("\nğŸ¯ ä½¿ç”¨è£é£¾å™¨é€²è¡Œæ€§èƒ½åˆ†æ...")
        
        @profile("image_processing_simulation")
        def image_processing_simulation():
            """æ¨¡æ“¬åœ–ç‰‡è™•ç†"""
            # æ¨¡æ“¬åœ–ç‰‡æ•¸æ“š
            image_data = list(range(100000))
            
            # æ¨¡æ“¬è™•ç†æ“ä½œ
            processed = []
            for pixel in image_data:
                processed.append(pixel * 1.1)
            
            return processed
        
        # åŸ·è¡Œè¢«è£é£¾çš„å‡½æ•¸
        result = image_processing_simulation()
        print(f"  âœ… åœ–ç‰‡è™•ç†å®Œæˆï¼Œè™•ç†äº† {len(result)} å€‹åƒç´ ")
        
        # æ¸¬è©¦ç•°æ­¥è£é£¾å™¨
        @profile("async_file_processing")
        async def async_file_processing():
            """æ¨¡æ“¬ç•°æ­¥æ–‡ä»¶è™•ç†"""
            files = ["file1.txt", "file2.txt", "file3.txt"]
            processed_files = []
            
            for file_name in files:
                await asyncio.sleep(0.02)  # æ¨¡æ“¬æ–‡ä»¶I/O
                processed_files.append(f"processed_{file_name}")
            
            return processed_files
        
        async_result = asyncio.run(async_file_processing())
        print(f"  âœ… ç•°æ­¥æ–‡ä»¶è™•ç†å®Œæˆï¼Œè™•ç†äº† {len(async_result)} å€‹æ–‡ä»¶")
        
        # ç”Ÿæˆæ€§èƒ½å ±å‘Š
        print("\nğŸ“ˆ ç”Ÿæˆæ€§èƒ½å ±å‘Š...")
        
        report = profiler.generate_report("document_processing_benchmark")
        
        print(f"ğŸ“Š æ€§èƒ½å ±å‘Šæ‘˜è¦:")
        print(f"  æ¸¬è©¦å¥—ä»¶: {report['suite_name']}")
        print(f"  ç¸½æ¸¬è©¦æ•¸: {report['total_tests']}")
        print(f"  æˆåŠŸç‡: {report['success_rate']:.1f}%")
        print(f"  å¹³å‡åŸ·è¡Œæ™‚é–“: {report['performance_stats']['execution_time']['avg']:.3f}s")
        print(f"  å¹³å‡å…§å­˜ä½¿ç”¨: {report['performance_stats']['memory_usage']['avg']:.2f}MB")
        print(f"  å¹³å‡ååé‡: {report['performance_stats']['throughput']['avg']:.2f} ops/sec")
        
        # æ¸¬è©¦åŸºæº–ç·šæ¯”è¼ƒ
        print("\nğŸ“Š æ¸¬è©¦åŸºæº–ç·šæ¯”è¼ƒ...")
        
        baseline = PerformanceMetrics(
            operation_name="baseline_reference",
            execution_time=0.1,
            memory_peak=10.0,
            throughput=10.0
        )
        
        comparison = profiler.compare_with_baseline("document_processing_benchmark", baseline)
        
        # å°å‡ºçµæœ
        print("\nğŸ’¾ å°å‡ºæ¸¬è©¦çµæœ...")
        
        export_path = Path("data/temp/performance_benchmark_results.json")
        export_path.parent.mkdir(parents=True, exist_ok=True)
        
        success = profiler.export_results(export_path)
        if success:
            print(f"  âœ… çµæœå·²å°å‡ºåˆ°: {export_path}")
        
        # æ¸¬è©¦å…¨å±€åˆ†æå™¨
        print("\nğŸŒ æ¸¬è©¦å…¨å±€åˆ†æå™¨...")
        
        global_profiler = get_profiler()
        
        with global_profiler.profile_operation("global_test_operation") as metrics:
            # æ¨¡æ“¬ä¸€äº›æ“ä½œ
            data = list(range(10000))
            result = sum(data)
            metrics.additional_metrics = {"sum_result": result}
        
        print(f"  âœ… å…¨å±€æ“ä½œå®Œæˆ: {metrics.execution_time:.3f}s")
        print(f"  ğŸ“Š é™„åŠ æŒ‡æ¨™: {metrics.additional_metrics}")
        
        # æ€§èƒ½æ¸¬è©¦ç¸½çµ
        print("\nğŸ‰ æ€§èƒ½åŸºæº–æ¸¬è©¦ç³»çµ±æ¼”ç¤ºå®Œæˆï¼")
        print("âœ… åŠŸèƒ½é©—è­‰:")
        print("  ğŸ” åŒæ­¥å‡½æ•¸åŸºæº–æ¸¬è©¦ âœ“")
        print("  âš¡ ç•°æ­¥å‡½æ•¸åŸºæº–æ¸¬è©¦ âœ“")
        print("  ğŸ¯ è£é£¾å™¨æ€§èƒ½åˆ†æ âœ“")
        print("  ğŸ“Š æ€§èƒ½å ±å‘Šç”Ÿæˆ âœ“")
        print("  ğŸ“ˆ åŸºæº–ç·šæ¯”è¼ƒ âœ“")
        print("  ğŸ’¾ çµæœå°å‡º âœ“")
        print("  ğŸŒ å…¨å±€åˆ†æå™¨ âœ“")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ¸¬è©¦éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = test_performance_benchmarks()
    exit(0 if success else 1)
