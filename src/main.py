"""
ä¸»æ‡‰ç”¨ç¨‹åºå…¥å£é»

æ™ºèƒ½æ–‡ä»¶è½‰æ›èˆ‡RAGçŸ¥è­˜åº«ç³»çµ±çš„æ ¸å¿ƒè™•ç†å¼•æ“ï¼Œ
å”èª¿æ–‡ä»¶è§£æã€åœ–æ–‡é—œè¯åˆ†æå’ŒMarkdownç”Ÿæˆçš„å®Œæ•´æµç¨‹ã€‚
"""

import os
import time
from typing import Dict, List, Any, Optional
from pathlib import Path
from datetime import datetime

from .config.settings import get_settings
from .config.logging_config import get_logger, PerformanceLogger
from .parsers import ParserFactory, ParsedContent
from .association import AssociationScorer
from .association.allen_logic import AllenLogicAnalyzer
from .association.caption_detector import CaptionDetector
from .association.spatial_analyzer import SpatialAnalyzer
from .association.semantic_analyzer import SemanticAnalyzer
from .association.association_optimizer import AssociationOptimizer, create_balanced_config
from .association.candidate_ranker import CandidateRanker
from .markdown import MarkdownGenerator
from .utils.file_utils import ensure_directory_exists, get_file_hash
from .utils.validation import validate_file_path, check_file_safety

logger = get_logger(__name__)


class DocumentProcessor:
    """
    æ–‡ä»¶è™•ç†æ ¸å¿ƒå¼•æ“
    
    å”èª¿æ•´å€‹æ–‡ä»¶è½‰æ›æµç¨‹ï¼Œå¾åŸå§‹æ–‡ä»¶åˆ°Markdownè¼¸å‡ºã€‚
    æ”¯æŒå¤šç¨®æ–‡ä»¶æ ¼å¼å’Œæ™ºèƒ½åœ–æ–‡é—œè¯åˆ†æã€‚
    """
    
    def __init__(self):
        """åˆå§‹åŒ–æ–‡ä»¶è™•ç†å™¨"""
        self.settings = get_settings()
        self.parser_factory = ParserFactory()
        self.markdown_generator = MarkdownGenerator()
        
        # åˆå§‹åŒ–é—œè¯åˆ†æçµ„ä»¶
        self.allen_analyzer = AllenLogicAnalyzer()
        self.caption_detector = CaptionDetector()
        self.spatial_analyzer = SpatialAnalyzer()
        self.semantic_analyzer = SemanticAnalyzer()
        self.association_scorer = AssociationScorer()
        
        # åˆå§‹åŒ–é—œè¯å„ªåŒ–å™¨
        self.association_optimizer = AssociationOptimizer(create_balanced_config())
        
        # åˆå§‹åŒ–å€™é¸æ’åºå™¨ï¼ˆæ™ºèƒ½é—œè¯é¸æ“‡ï¼‰
        self.candidate_ranker = CandidateRanker(
            caption_detector=self.caption_detector,
            semantic_analyzer=self.semantic_analyzer
        )
        
        # æ€§èƒ½ç›£æ§
        self.performance_logger = PerformanceLogger()
        
        logger.info("æ–‡ä»¶è™•ç†å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def process_document(
        self,
        file_path: str,
        output_dir: Optional[str] = None,
        template_name: str = "enhanced.md.j2",
        save_associations: bool = True
    ) -> Dict[str, Any]:
        """
        è™•ç†å–®å€‹æ–‡æª”
        
        Args:
            file_path: è¼¸å…¥æ–‡ä»¶è·¯å¾‘
            output_dir: è¼¸å‡ºç›®éŒ„ï¼ˆå¯é¸ï¼‰
            template_name: ä½¿ç”¨çš„æ¨¡æ¿åç¨±
            save_associations: æ˜¯å¦ä¿å­˜é—œè¯åˆ†æçµæœ
            
        Returns:
            Dict: è™•ç†çµæœï¼ŒåŒ…å«ç”Ÿæˆçš„æ–‡ä»¶è·¯å¾‘å’Œçµ±è¨ˆä¿¡æ¯
        """
        
        start_time = time.time()
        
        try:
            # 1. æ–‡ä»¶é©—è­‰
            logger.info(f"é–‹å§‹è™•ç†æ–‡æª”: {file_path}")
            
            if not validate_file_path(file_path):
                raise ValueError(f"æ–‡ä»¶è·¯å¾‘ç„¡æ•ˆ: {file_path}")
            
            is_safe, warnings = check_file_safety(file_path)
            if not is_safe:
                logger.warning(f"æ–‡ä»¶å®‰å…¨æª¢æŸ¥è­¦å‘Š: {warnings}")
            
            # 2. æ–‡ä»¶è§£æ
            with self.performance_logger.measure("file_parsing"):
                parsed_content = self._parse_file(file_path)
            
            logger.info(
                f"æ–‡ä»¶è§£æå®Œæˆ - æ–‡æœ¬å¡Š: {len(parsed_content.text_blocks)}, "
                f"åœ–ç‰‡: {len(parsed_content.images)}, "
                f"è¡¨æ ¼: {len(parsed_content.tables)}"
            )
            
            # 3. åœ–æ–‡é—œè¯åˆ†æ
            with self.performance_logger.measure("association_analysis"):
                associations = self._analyze_associations(parsed_content)
            
            logger.info(f"é—œè¯åˆ†æå®Œæˆ - ç™¼ç¾ {len(associations)} å€‹é—œè¯é—œä¿‚")
            
            # 4. ç”ŸæˆMarkdown
            with self.performance_logger.measure("markdown_generation"):
                markdown_content = self._generate_markdown(
                    parsed_content, associations, template_name
                )
            
            # 5. ä¿å­˜çµæœ
            output_files = self._save_results(
                parsed_content, associations, markdown_content,
                output_dir, save_associations
            )
            
            # 6. çµ±è¨ˆä¿¡æ¯
            processing_time = time.time() - start_time
            stats = self._collect_statistics(
                parsed_content, associations, processing_time
            )
            
            logger.info(f"æ–‡æª”è™•ç†å®Œæˆï¼Œè€—æ™‚: {processing_time:.2f}ç§’")
            
            return {
                "success": True,
                "output_files": output_files,
                "statistics": stats,
                "processing_time": processing_time
            }
            
        except Exception as e:
            logger.error(f"æ–‡æª”è™•ç†å¤±æ•—: {e}")
            return {
                "success": False,
                "error": str(e),
                "processing_time": time.time() - start_time
            }
    
    def _parse_file(self, file_path: str) -> ParsedContent:
        """è§£ææ–‡ä»¶å…§å®¹"""
        
        file_extension = Path(file_path).suffix.lower()
        parser = self.parser_factory.get_parser(file_extension)
        
        if not parser:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_extension}")
        
        return parser.parse(file_path)
    
    def _analyze_associations(self, parsed_content: ParsedContent) -> List[Dict[str, Any]]:
        """åˆ†æåœ–æ–‡é—œè¯é—œä¿‚ - ä½¿ç”¨æ™ºèƒ½å€™é¸æ’åºç­–ç•¥"""
        
        associations = []
        
        # æ”¶é›†æ‰€æœ‰å…ƒç´ ä¾›å¢å¼·ç©ºé–“åˆ†æä½¿ç”¨
        self._current_all_elements = parsed_content.text_blocks + parsed_content.images
        
        # æº–å‚™ä¸Šä¸‹æ–‡ä¿¡æ¯
        context_info = {
            'all_elements': self._current_all_elements,
            # å·²ç§»é™¤ç¡¬ç·¨ç¢¼layout_typeï¼Œè®“å¢å¼·ç©ºé–“åˆ†æè‡ªå‹•æª¢æ¸¬ä½ˆå±€
        }
        
        logger.info(f"é–‹å§‹æ™ºèƒ½åœ–æ–‡é—œè¯åˆ†æ - åœ–ç‰‡æ•¸: {len(parsed_content.images)}, æ–‡æœ¬å¡Šæ•¸: {len(parsed_content.text_blocks)}")
        
        # ğŸ¯ æ ¸å¿ƒæ”¹é€²ï¼šåœ–ç‰‡å„ªå…ˆçš„æ™ºèƒ½å€™é¸æ’åºç­–ç•¥
        for image in parsed_content.images:
            try:
                # æº–å‚™å€™é¸æ–‡æœ¬åˆ—è¡¨ï¼ˆæ‰€æœ‰æ–‡æœ¬å¡Šéƒ½æ˜¯æ½›åœ¨å€™é¸ï¼‰
                text_candidates = [
                    {
                        'id': text_block.id,
                        'content': text_block.content,
                        'bbox': text_block.bbox
                    }
                    for text_block in parsed_content.text_blocks
                ]
                
                # ä½¿ç”¨CandidateRankeré€²è¡Œæ™ºèƒ½æ’åº
                ranked_candidates = self.candidate_ranker.rank_candidates(
                    text_candidates=text_candidates,
                    image_bbox=image.bbox,
                    image_content=getattr(image, 'description', None),
                    context_info=context_info
                )
                
                # é¸æ“‡æ¨è–¦çš„å€™é¸é€²è¡Œè©³ç´°é—œè¯åˆ†æ
                threshold = self.settings.association.min_association_score
                
                for ranked_candidate in ranked_candidates:
                    # åªè™•ç†æ¨è–¦çš„å€™é¸æˆ–é«˜åˆ†å€™é¸
                    if (ranked_candidate.is_recommended or 
                        ranked_candidate.scores.final_score >= threshold):
                        
                        # å¾ranked_candidateè½‰æ›å›text_blockæ ¼å¼
                        text_block = next(
                            (tb for tb in parsed_content.text_blocks 
                             if tb.id == ranked_candidate.text_id), 
                            None
                        )
                        
                        if text_block:
                            # åŸ·è¡Œå®Œæ•´çš„é—œè¯åˆ†æ
                            association_result = self._perform_association_analysis(
                                text_block, image, parsed_content
                            )
                            
                            # èåˆCandidateRankerçš„æ’åºä¿¡æ¯
                            association_result.update({
                                'candidate_rank': ranked_candidate.rank,
                                'candidate_quality': ranked_candidate.scores.quality.value,
                                'candidate_reasoning': ranked_candidate.reasoning,
                                'is_candidate_recommended': ranked_candidate.is_recommended
                            })
                            
                            associations.append(association_result)
                
                logger.debug(f"åœ–ç‰‡ {image.id} å®Œæˆå€™é¸æ’åºï¼Œæ‰¾åˆ° {len([c for c in ranked_candidates if c.is_recommended])} å€‹æ¨è–¦é—œè¯")
                
            except Exception as e:
                logger.warning(f"åœ–ç‰‡ {image.id} çš„æ™ºèƒ½é—œè¯åˆ†æå¤±æ•—: {e}")
        
        # ğŸ”§ é—œè¯å„ªåŒ– - å»é‡ã€éæ¿¾å’Œè³ªé‡æå‡
        logger.info(f"é–‹å§‹é—œè¯å„ªåŒ– - åŸå§‹é—œè¯æ•¸: {len(associations)}")
        
        optimized_associations = self.association_optimizer.optimize_associations(
            associations=associations,
            images=parsed_content.images,
            text_blocks=parsed_content.text_blocks
        )
        
        logger.info(f"æ™ºèƒ½é—œè¯åˆ†æå®Œæˆ - å„ªåŒ–å¾Œé—œè¯æ•¸: {len(optimized_associations)}")
        logger.info(f"é—œè¯æ¸›å°‘ç‡: {((len(associations) - len(optimized_associations)) / len(associations) * 100):.1f}%" if associations else "0%")
        
        return optimized_associations
    
    def _perform_association_analysis(self, text_block, image, parsed_content: ParsedContent = None) -> Dict[str, Any]:
        """åŸ·è¡Œå–®å€‹æ–‡æœ¬å¡Šå’Œåœ–ç‰‡çš„é—œè¯åˆ†æ"""
        
        # 1. Captionæª¢æ¸¬
        caption_matches = self.caption_detector.detect_captions(
            text_block.content, text_block.bbox, image.bbox
        )
        caption_score = max((match.confidence for match in caption_matches), default=0.0)
        
        # 2. ç©ºé–“é—œä¿‚åˆ†æ
        spatial_features = self.spatial_analyzer.calculate_spatial_features(
            text_block.bbox, image.bbox
        )
        
        # 3. èªç¾©ç›¸ä¼¼åº¦åˆ†æ
        semantic_score = self.semantic_analyzer.calculate_similarity(
            text_block.content, 
            image.alt_text or f"Image {image.id}"
        )
        
        # 4. å¢å¼·çš„ç©ºé–“é—œä¿‚åˆ†æï¼ˆä½¿ç”¨æ”¹é€²çš„ç®—æ³•ï¼‰
        try:
            # å‰µå»ºä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆç”¨æ–¼å¢å¼·ç©ºé–“åˆ†æï¼‰
            all_elements = getattr(self, '_current_all_elements', [])
            if parsed_content:
                all_elements = parsed_content.text_blocks + parsed_content.images
            
            context_info = {
                'all_elements': all_elements,
                # ç§»é™¤ç¡¬ç·¨ç¢¼çš„layout_typeï¼Œè®“å¢å¼·ç©ºé–“åˆ†æè‡ªå‹•æª¢æ¸¬ä½ˆå±€
                'text_content': text_block.content  # å‚³éæ–‡æœ¬å…§å®¹ä»¥ä¾›åˆ†æ
            }
            
            # ä½¿ç”¨å¢å¼·çš„ç©ºé–“åˆ†æï¼ˆåƒ…ç”¨æ–¼ç©ºé–“é—œä¿‚éƒ¨åˆ†ï¼‰
            enhanced_result = self.spatial_analyzer.calculate_enhanced_spatial_features(
                text_block.bbox, image.bbox, context_info
            )
            
            # æå–ç©ºé–“åˆ†æçµæœ
            enhanced_spatial_score = enhanced_result['final_score']  # é€™æ˜¯æ”¹é€²å¾Œçš„ç©ºé–“åˆ†æ•¸
            
            # å¯é¸ï¼šè¨˜éŒ„è©³ç´°åˆ†æä¿¡æ¯ä¾›èª¿è©¦
            logger.debug(f"å¢å¼·ç©ºé–“åˆ†æè©³æƒ… - é—œä¿‚: {enhanced_result['details']['relationship']}, "
                        f"ç©ºé–“åˆ†æ•¸: {enhanced_spatial_score:.3f}")
            
        except Exception as e:
            # å‚™é¸æ–¹æ¡ˆï¼šä½¿ç”¨å‚³çµ±ç©ºé–“åˆ†ææ–¹æ³•
            logger.warning(f"å¢å¼·ç©ºé–“åˆ†æå¤±æ•—ï¼Œä½¿ç”¨å‚³çµ±æ–¹æ³•: {e}")
            
            if spatial_features:
                # ä¼°ç®—é é¢å°ºå¯¸ï¼ˆä½¿ç”¨æ‰€æœ‰å…ƒç´ çš„æœ€å¤§åº§æ¨™ï¼‰
                page_width = max(text_block.bbox.right, image.bbox.right)
                page_height = max(text_block.bbox.bottom, image.bbox.bottom)
                page_diagonal = (page_width ** 2 + page_height ** 2) ** 0.5
                
                # ä½¿ç”¨å‚³çµ±çš„ç©ºé–“åˆ†æ•¸è¨ˆç®—
                enhanced_spatial_score = 1.0 - min(spatial_features.center_distance / (page_diagonal * 0.5), 1.0)
            else:
                enhanced_spatial_score = 0.0
        
        # 5. ç¶œåˆè©•åˆ† - æŒ‰ç…§é …ç›®è¦å‰‡çš„æ¬Šé‡æ¨¡å‹
        # Caption 40% + Spatial 30% + Semantic 15% + Layout 10% + Proximity 5%
        
        # ä½ˆå±€å’Œè·é›¢åˆ†æ•¸ï¼ˆå¾å¢å¼·ç©ºé–“åˆ†æä¸­ç²å–ï¼‰
        if 'enhanced_result' in locals() and enhanced_result:
            layout_score = enhanced_result['component_scores'].get('alignment', 0.5)
            proximity_score = enhanced_result['component_scores'].get('distance', 0.5)
        else:
            layout_score = spatial_features.alignment_score if spatial_features else 0.5
            proximity_score = 0.5  # é è¨­å€¼
        
        final_score, details = self.association_scorer.calculate_simple_score(
            caption_score=caption_score,
            spatial_score=enhanced_spatial_score,  # ä½¿ç”¨å¢å¼·çš„ç©ºé–“åˆ†æ•¸
            semantic_score=semantic_score,
            layout_score=layout_score,
            proximity_score=proximity_score
        )
        
        return {
            "text_block_id": text_block.id,
            "image_id": image.id,
            "final_score": final_score,
            "caption_score": caption_score,
            "spatial_score": enhanced_spatial_score,  # ä½¿ç”¨å¢å¼·çš„ç©ºé–“åˆ†æ•¸
            "semantic_score": semantic_score,
            "layout_score": layout_score,
            "proximity_score": proximity_score,
            "spatial_relation": "enhanced",  # ä½¿ç”¨å¢å¼·çš„ç©ºé–“åˆ†æ
            "association_type": "caption" if caption_score > 0.5 else "spatial",
            "details": details,
            "enhanced_spatial_details": enhanced_result.get('details', {}) if 'enhanced_result' in locals() else {},
            "spatial_features": {
                "center_distance": spatial_features.center_distance if spatial_features else 0,
                "alignment_score": spatial_features.alignment_score if spatial_features else 0,
                "min_distance": spatial_features.min_distance if spatial_features else 0
            }
        }
    
    def _generate_markdown(
        self, 
        parsed_content: ParsedContent, 
        associations: List[Dict[str, Any]],
        template_name: str
    ) -> str:
        """ç”ŸæˆMarkdownå…§å®¹"""
        
        return self.markdown_generator.generate(
            parsed_content=parsed_content,
            associations=associations,
            template_name=template_name,
            include_metadata=True
        )
    
    def _save_results(
        self,
        parsed_content: ParsedContent,
        associations: List[Dict[str, Any]],
        markdown_content: str,
        output_dir: Optional[str],
        save_associations: bool
    ) -> Dict[str, str]:
        """ä¿å­˜è™•ç†çµæœ"""
        
        if not output_dir:
            output_dir = "./output"
        
        ensure_directory_exists(output_dir)
        
        # ç”Ÿæˆæ–‡ä»¶å
        base_name = Path(parsed_content.metadata.filename).stem
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        output_files = {}
        
        # ä¿å­˜Markdownæ–‡ä»¶
        markdown_path = Path(output_dir) / f"{base_name}_{timestamp}.md"
        with open(markdown_path, 'w', encoding='utf-8') as f:
            f.write(markdown_content)
        output_files["markdown"] = str(markdown_path)
        
        # ä¿å­˜é—œè¯åˆ†æçµæœ
        if save_associations:
            import json
            from enum import Enum
            
            # åºåˆ—åŒ–åŠ©æ‰‹å‡½æ•¸
            def serialize_for_json(obj):
                if isinstance(obj, Enum):
                    return obj.value
                elif hasattr(obj, '__dict__'):
                    return {k: serialize_for_json(v) for k, v in obj.__dict__.items()}
                elif isinstance(obj, (list, tuple)):
                    return [serialize_for_json(item) for item in obj]
                elif isinstance(obj, dict):
                    return {k: serialize_for_json(v) for k, v in obj.items()}
                else:
                    return obj
            
            serializable_associations = serialize_for_json(associations)
            
            associations_path = Path(output_dir) / f"{base_name}_{timestamp}_associations.json"
            with open(associations_path, 'w', encoding='utf-8') as f:
                json.dump(serializable_associations, f, ensure_ascii=False, indent=2)
            output_files["associations"] = str(associations_path)
        
        return output_files
    
    def _collect_statistics(
        self,
        parsed_content: ParsedContent,
        associations: List[Dict[str, Any]],
        processing_time: float
    ) -> Dict[str, Any]:
        """æ”¶é›†è™•ç†çµ±è¨ˆä¿¡æ¯"""
        
        # è¨ˆç®—é—œè¯çµ±è¨ˆ
        high_quality_associations = [
            a for a in associations if a["final_score"] >= 0.7
        ]
        
        caption_associations = [
            a for a in associations if a["association_type"] == "caption"
        ]
        
        return {
            "processing_time": processing_time,
            "total_text_blocks": len(parsed_content.text_blocks),
            "total_images": len(parsed_content.images),
            "total_tables": len(parsed_content.tables),
            "total_associations": len(associations),
            "high_quality_associations": len(high_quality_associations),
            "caption_associations": len(caption_associations),
            "average_association_score": (
                sum(a["final_score"] for a in associations) / len(associations)
                if associations else 0.0
            ),
            "performance_metrics": self.performance_logger.get_summary()
        }


def main():
    """ä¸»å‡½æ•¸ - å‘½ä»¤è¡Œç•Œé¢"""
    
    import argparse
    
    parser = argparse.ArgumentParser(
        description="æ™ºèƒ½æ–‡ä»¶è½‰æ›èˆ‡RAGçŸ¥è­˜åº«ç³»çµ±"
    )
    
    parser.add_argument(
        "input_file",
        help="è¼¸å…¥æ–‡ä»¶è·¯å¾‘"
    )
    
    parser.add_argument(
        "-o", "--output",
        help="è¼¸å‡ºç›®éŒ„",
        default="./output"
    )
    
    parser.add_argument(
        "-t", "--template",
        help="ä½¿ç”¨çš„æ¨¡æ¿",
        choices=["basic.md.j2", "enhanced.md.j2"],
        default="enhanced.md.j2"
    )
    
    parser.add_argument(
        "--no-associations",
        action="store_true",
        help="ä¸ä¿å­˜é—œè¯åˆ†æçµæœ"
    )
    
    args = parser.parse_args()
    
    try:
        processor = DocumentProcessor()
        
        result = processor.process_document(
            file_path=args.input_file,
            output_dir=args.output,
            template_name=args.template,
            save_associations=not args.no_associations
        )
        
        if result["success"]:
            print("âœ… è™•ç†æˆåŠŸï¼")
            print(f"ğŸ“Š çµ±è¨ˆä¿¡æ¯:")
            stats = result["statistics"]
            print(f"  - è™•ç†æ™‚é–“: {stats['processing_time']:.2f}ç§’")
            print(f"  - æ–‡æœ¬å¡Š: {stats['total_text_blocks']}")
            print(f"  - åœ–ç‰‡: {stats['total_images']}")
            print(f"  - é—œè¯é—œä¿‚: {stats['total_associations']}")
            print(f"  - é«˜è³ªé‡é—œè¯: {stats['high_quality_associations']}")
            
            print(f"ğŸ“ è¼¸å‡ºæ–‡ä»¶:")
            for file_type, file_path in result["output_files"].items():
                print(f"  - {file_type}: {file_path}")
        else:
            print(f"âŒ è™•ç†å¤±æ•—: {result['error']}")
            return 1
    
    except Exception as e:
        logger.error(f"ä¸»ç¨‹åºåŸ·è¡Œå¤±æ•—: {e}")
        print(f"âŒ åŸ·è¡Œå¤±æ•—: {e}")
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main())
