"""
Azure OpenAIæœå‹™

æä¾›èˆ‡Azure OpenAIçš„é›†æˆåŠŸèƒ½ï¼ŒåŒ…æ‹¬Chatã€Embeddingç­‰æ¨¡å‹èª¿ç”¨ã€‚
"""

import asyncio
import time
from typing import List, Dict, Any, Optional, AsyncGenerator
from dataclasses import dataclass
from datetime import datetime

from openai import AsyncAzureOpenAI
from openai.types.chat import ChatCompletion
from openai.types import CreateEmbeddingResponse
import tiktoken
from loguru import logger

from ..config.settings import AzureOpenAISettings, get_settings


@dataclass
class TokenUsage:
    """Tokenä½¿ç”¨çµ±è¨ˆ"""
    prompt_tokens: int
    completion_tokens: int = 0
    total_tokens: int = 0
    
    def __post_init__(self):
        if self.total_tokens == 0:
            self.total_tokens = self.prompt_tokens + self.completion_tokens


@dataclass
class ChatMessage:
    """èŠå¤©æ¶ˆæ¯"""
    role: str  # "user", "assistant", "system"
    content: str
    name: Optional[str] = None


class RateLimiter:
    """é€Ÿç‡é™åˆ¶å™¨"""
    
    def __init__(self, max_requests_per_minute: int, max_tokens_per_minute: int):
        self.max_requests_per_minute = max_requests_per_minute
        self.max_tokens_per_minute = max_tokens_per_minute
        
        self.request_times: List[float] = []
        self.token_usage: List[tuple[float, int]] = []  # (timestamp, tokens)
        
        self.logger = logger.bind(module="rate_limiter")
    
    async def wait_if_needed(self, estimated_tokens: int = 0):
        """å¦‚æœéœ€è¦ï¼Œç­‰å¾…ä»¥é¿å…è¶…éé€Ÿç‡é™åˆ¶"""
        current_time = time.time()
        
        # æ¸…ç†1åˆ†é˜å‰çš„è¨˜éŒ„
        self._cleanup_old_records(current_time)
        
        # æª¢æŸ¥è«‹æ±‚é€Ÿç‡
        if len(self.request_times) >= self.max_requests_per_minute:
            wait_time = 60 - (current_time - self.request_times[0])
            if wait_time > 0:
                self.logger.warning(f"â° è«‹æ±‚é€Ÿç‡é™åˆ¶ï¼Œç­‰å¾… {wait_time:.1f} ç§’")
                await asyncio.sleep(wait_time)
                self._cleanup_old_records(time.time())
        
        # æª¢æŸ¥Tokené€Ÿç‡
        current_tokens = sum(tokens for _, tokens in self.token_usage)
        if current_tokens + estimated_tokens > self.max_tokens_per_minute:
            wait_time = 60 - (current_time - self.token_usage[0][0]) if self.token_usage else 0
            if wait_time > 0:
                self.logger.warning(f"ğŸ¯ Tokené€Ÿç‡é™åˆ¶ï¼Œç­‰å¾… {wait_time:.1f} ç§’")
                await asyncio.sleep(wait_time)
                self._cleanup_old_records(time.time())
        
        # è¨˜éŒ„ç•¶å‰è«‹æ±‚
        self.request_times.append(current_time)
        if estimated_tokens > 0:
            self.token_usage.append((current_time, estimated_tokens))
    
    def _cleanup_old_records(self, current_time: float):
        """æ¸…ç†1åˆ†é˜å‰çš„è¨˜éŒ„"""
        cutoff_time = current_time - 60
        self.request_times = [t for t in self.request_times if t > cutoff_time]
        self.token_usage = [(t, tokens) for t, tokens in self.token_usage if t > cutoff_time]
    
    def record_usage(self, tokens_used: int):
        """è¨˜éŒ„å¯¦éš›ä½¿ç”¨çš„Tokenæ•¸"""
        current_time = time.time()
        self.token_usage.append((current_time, tokens_used))


class AzureOpenAIService:
    """
    Azure OpenAIæœå‹™é¡
    
    æä¾›èˆ‡Azure OpenAIçš„å®Œæ•´é›†æˆåŠŸèƒ½ã€‚
    """
    
    def __init__(self, settings: Optional[AzureOpenAISettings] = None):
        """
        åˆå§‹åŒ–Azure OpenAIæœå‹™
        
        Args:
            settings: Azure OpenAIé…ç½®ï¼Œå¦‚æœç‚ºNoneå‰‡ä½¿ç”¨é»˜èªé…ç½®
        """
        if settings is None:
            app_settings = get_settings()
            settings = app_settings.azure_openai
        
        self.settings = settings
        self.logger = logger.bind(module="azure_openai")
        
        # é©—è­‰é…ç½®
        if not settings.endpoint or not settings.api_key:
            raise ValueError("Azure OpenAI endpointå’Œapi_keyå¿…é ˆé…ç½®")
        
        # åˆå§‹åŒ–å®¢æˆ¶ç«¯
        self.client = AsyncAzureOpenAI(
            azure_endpoint=settings.endpoint,
            api_key=settings.api_key,
            api_version=settings.api_version,
            timeout=settings.request_timeout,
            max_retries=settings.max_retries
        )
        
        # åˆå§‹åŒ–é€Ÿç‡é™åˆ¶å™¨
        self.rate_limiter = RateLimiter(
            max_requests_per_minute=settings.max_requests_per_minute,
            max_tokens_per_minute=settings.max_tokens_per_minute
        )
        
        # åˆå§‹åŒ–Tokenè¨ˆæ•¸å™¨
        try:
            self.encoding = tiktoken.get_encoding("cl100k_base")  # GPT-4ä½¿ç”¨çš„ç·¨ç¢¼
        except Exception as e:
            self.logger.warning(f"ç„¡æ³•åˆå§‹åŒ–tiktokenç·¨ç¢¼å™¨: {e}")
            self.encoding = None
        
        self.logger.info(f"âœ… Azure OpenAIæœå‹™å·²åˆå§‹åŒ–")
        self.logger.info(f"ğŸ“ ç«¯é»: {settings.endpoint}")
        self.logger.info(f"ğŸ¤– Chatæ¨¡å‹: {settings.chat_deployment_name}")
        self.logger.info(f"ğŸ§  Embeddingæ¨¡å‹: {settings.embedding_deployment_name}")
    
    def count_tokens(self, text: str) -> int:
        """
        è¨ˆç®—æ–‡æœ¬çš„Tokenæ•¸é‡
        
        Args:
            text: è¦è¨ˆç®—çš„æ–‡æœ¬
            
        Returns:
            Tokenæ•¸é‡
        """
        if self.encoding is None:
            # ç°¡å–®ä¼°ç®—ï¼š1 token â‰ˆ 4 characters
            return len(text) // 4
        
        try:
            return len(self.encoding.encode(text))
        except Exception:
            # å‚™ç”¨ä¼°ç®—
            return len(text) // 4
    
    def estimate_chat_tokens(self, messages: List[ChatMessage], model: str = None) -> int:
        """
        ä¼°ç®—èŠå¤©è«‹æ±‚çš„Tokenæ•¸é‡
        
        Args:
            messages: èŠå¤©æ¶ˆæ¯åˆ—è¡¨
            model: æ¨¡å‹åç¨±
            
        Returns:
            ä¼°ç®—çš„Tokenæ•¸é‡
        """
        # åŸºç¤Tokenæ•¸ï¼ˆç³»çµ±é–‹éŠ·ï¼‰
        tokens = 3
        
        for message in messages:
            tokens += 3  # æ¯æ¢æ¶ˆæ¯çš„åŸºç¤é–‹éŠ·
            tokens += self.count_tokens(message.content)
            if message.name:
                tokens += self.count_tokens(message.name)
        
        tokens += 3  # åŠ©æ‰‹å›å¾©çš„åŸºç¤é–‹éŠ·
        
        return tokens
    
    async def chat_completion(
        self,
        messages: List[ChatMessage],
        model: Optional[str] = None,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        stream: bool = False,
        **kwargs
    ) -> Dict[str, Any]:
        """
        ç™¼é€èŠå¤©å®Œæˆè«‹æ±‚
        
        Args:
            messages: èŠå¤©æ¶ˆæ¯åˆ—è¡¨
            model: æ¨¡å‹åç¨±ï¼Œé»˜èªä½¿ç”¨é…ç½®çš„chat_deployment_name
            temperature: æº«åº¦åƒæ•¸
            max_tokens: æœ€å¤§Tokenæ•¸
            stream: æ˜¯å¦æµå¼éŸ¿æ‡‰
            **kwargs: å…¶ä»–åƒæ•¸
            
        Returns:
            èŠå¤©å®ŒæˆéŸ¿æ‡‰
        """
        if model is None:
            model = self.settings.chat_deployment_name
        
        if temperature is None:
            temperature = self.settings.default_temperature
        
        if max_tokens is None:
            max_tokens = self.settings.default_max_tokens
        
        # ä¼°ç®—Tokenä½¿ç”¨é‡
        estimated_tokens = self.estimate_chat_tokens(messages, model)
        estimated_tokens += max_tokens  # åŠ ä¸Šå›å¾©Tokenä¼°ç®—
        
        # ç­‰å¾…é€Ÿç‡é™åˆ¶
        await self.rate_limiter.wait_if_needed(estimated_tokens)
        
        # æº–å‚™æ¶ˆæ¯
        openai_messages = [
            {"role": msg.role, "content": msg.content}
            for msg in messages
        ]
        
        start_time = time.time()
        
        try:
            self.logger.info(f"ğŸ¤– ç™¼é€Chatè«‹æ±‚ - æ¨¡å‹: {model}, æ¶ˆæ¯æ•¸: {len(messages)}")
            
            # ç™¼é€è«‹æ±‚
            response = await self.client.chat.completions.create(
                model=model,
                messages=openai_messages,
                temperature=temperature,
                max_tokens=max_tokens,
                stream=stream,
                **kwargs
            )
            
            processing_time = time.time() - start_time
            
            if stream:
                # æµå¼éŸ¿æ‡‰
                return {
                    "type": "stream",
                    "response": response,
                    "processing_time": processing_time
                }
            else:
                # æ¨™æº–éŸ¿æ‡‰
                usage = TokenUsage(
                    prompt_tokens=response.usage.prompt_tokens,
                    completion_tokens=response.usage.completion_tokens,
                    total_tokens=response.usage.total_tokens
                )
                
                # è¨˜éŒ„å¯¦éš›Tokenä½¿ç”¨é‡
                self.rate_limiter.record_usage(usage.total_tokens)
                
                self.logger.info(
                    f"âœ… Chatè«‹æ±‚å®Œæˆ - "
                    f"ç”¨æ™‚: {processing_time:.2f}s, "
                    f"Token: {usage.total_tokens} "
                    f"(æç¤º: {usage.prompt_tokens}, å®Œæˆ: {usage.completion_tokens})"
                )
                
                return {
                    "type": "completion",
                    "id": response.id,
                    "model": response.model,
                    "message": response.choices[0].message.content,
                    "usage": usage,
                    "processing_time": processing_time,
                    "created_at": datetime.utcnow()
                }
        
        except Exception as e:
            processing_time = time.time() - start_time
            self.logger.error(f"âŒ Chatè«‹æ±‚å¤±æ•—: {str(e)} (ç”¨æ™‚: {processing_time:.2f}s)")
            raise
    
    async def create_embeddings(
        self,
        texts: List[str],
        model: Optional[str] = None,
        batch_size: int = 100
    ) -> Dict[str, Any]:
        """
        å‰µå»ºæ–‡æœ¬åµŒå…¥å‘é‡
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            model: æ¨¡å‹åç¨±ï¼Œé»˜èªä½¿ç”¨é…ç½®çš„embedding_deployment_name
            batch_size: æ‰¹é‡å¤§å°
            
        Returns:
            åµŒå…¥å‘é‡éŸ¿æ‡‰
        """
        if model is None:
            model = self.settings.embedding_deployment_name
        
        if not texts:
            raise ValueError("æ–‡æœ¬åˆ—è¡¨ä¸èƒ½ç‚ºç©º")
        
        all_embeddings = []
        total_tokens = 0
        start_time = time.time()
        
        # åˆ†æ‰¹è™•ç†
        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i:i + batch_size]
            
            # ä¼°ç®—Tokenä½¿ç”¨é‡
            estimated_tokens = sum(self.count_tokens(text) for text in batch_texts)
            
            # ç­‰å¾…é€Ÿç‡é™åˆ¶
            await self.rate_limiter.wait_if_needed(estimated_tokens)
            
            try:
                self.logger.info(f"ğŸ§  å‰µå»ºEmbedding - æ‰¹æ¬¡: {i//batch_size + 1}, æ–‡æœ¬æ•¸: {len(batch_texts)}")
                
                # ç™¼é€è«‹æ±‚
                response = await self.client.embeddings.create(
                    model=model,
                    input=batch_texts
                )
                
                # è¨˜éŒ„Tokenä½¿ç”¨é‡
                batch_tokens = response.usage.total_tokens
                total_tokens += batch_tokens
                self.rate_limiter.record_usage(batch_tokens)
                
                # æ”¶é›†çµæœ
                for j, embedding_data in enumerate(response.data):
                    all_embeddings.append({
                        "text": batch_texts[j],
                        "embedding": embedding_data.embedding,
                        "index": i + j
                    })
                
                self.logger.debug(f"âœ… æ‰¹æ¬¡å®Œæˆ - Token: {batch_tokens}")
                
            except Exception as e:
                self.logger.error(f"âŒ Embeddingæ‰¹æ¬¡å¤±æ•—: {str(e)}")
                raise
        
        processing_time = time.time() - start_time
        
        self.logger.info(
            f"âœ… Embeddingå‰µå»ºå®Œæˆ - "
            f"æ–‡æœ¬æ•¸: {len(texts)}, "
            f"ç”¨æ™‚: {processing_time:.2f}s, "
            f"ç¸½Token: {total_tokens}"
        )
        
        return {
            "model": model,
            "embeddings": all_embeddings,
            "total_tokens": total_tokens,
            "processing_time": processing_time,
            "created_at": datetime.utcnow()
        }
    
    async def chat_completion_stream(
        self,
        messages: List[ChatMessage],
        model: Optional[str] = None,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        **kwargs
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """
        æµå¼èŠå¤©å®Œæˆ
        
        Args:
            messages: èŠå¤©æ¶ˆæ¯åˆ—è¡¨
            model: æ¨¡å‹åç¨±
            temperature: æº«åº¦åƒæ•¸
            max_tokens: æœ€å¤§Tokenæ•¸
            **kwargs: å…¶ä»–åƒæ•¸
            
        Yields:
            æµå¼éŸ¿æ‡‰æ•¸æ“š
        """
        response_data = await self.chat_completion(
            messages=messages,
            model=model,
            temperature=temperature,
            max_tokens=max_tokens,
            stream=True,
            **kwargs
        )
        
        if response_data["type"] != "stream":
            raise ValueError("æœŸæœ›æµå¼éŸ¿æ‡‰ä½†æ”¶åˆ°æ¨™æº–éŸ¿æ‡‰")
        
        async for chunk in response_data["response"]:
            if chunk.choices and chunk.choices[0].delta.content:
                yield {
                    "id": chunk.id,
                    "content": chunk.choices[0].delta.content,
                    "finish_reason": chunk.choices[0].finish_reason
                }
    
    async def health_check(self) -> Dict[str, Any]:
        """
        å¥åº·æª¢æŸ¥
        
        Returns:
            å¥åº·ç‹€æ…‹ä¿¡æ¯
        """
        try:
            # ç™¼é€ä¸€å€‹ç°¡å–®çš„æ¸¬è©¦è«‹æ±‚
            test_messages = [ChatMessage(role="user", content="Hello")]
            response = await self.chat_completion(
                messages=test_messages,
                max_tokens=5
            )
            
            return {
                "status": "healthy",
                "endpoint": self.settings.endpoint,
                "chat_model": self.settings.chat_deployment_name,
                "embedding_model": self.settings.embedding_deployment_name,
                "test_response_time": response["processing_time"]
            }
        
        except Exception as e:
            return {
                "status": "unhealthy",
                "error": str(e),
                "endpoint": self.settings.endpoint
            }


# å…¨å±€æœå‹™å¯¦ä¾‹ï¼ˆå–®ä¾‹æ¨¡å¼ï¼‰
_azure_openai_service: Optional[AzureOpenAIService] = None


def get_azure_openai_service() -> AzureOpenAIService:
    """
    ç²å–Azure OpenAIæœå‹™å¯¦ä¾‹ï¼ˆå–®ä¾‹æ¨¡å¼ï¼‰
    
    Returns:
        Azure OpenAIæœå‹™å¯¦ä¾‹
    """
    global _azure_openai_service
    
    if _azure_openai_service is None:
        _azure_openai_service = AzureOpenAIService()
    
    return _azure_openai_service
